Fri 06 Nov 2020 01:45:46 PM MST

[ ] Project.toml
[ ] tests

Tue 01 May 2018 02:07:41 PM PDT

Dang - not even a billion will work, haha

N = 1000000000 = 1000000000
Progress: 100%|█████████████████████████████████████████| Time: 1:16:15
tree.v[best_node] = -4.788946905946339

Should be -5.33

Tue 01 May 2018 08:52:06 AM PDT

zsunberg@bethpage:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 54 multilane/compare.jl
cor = 0.0 = 0.0
lambda = 2.0 = 2.0
N = 1000 = 1000
n_iters = 1000000 = 1000000
max_time = 1.0 = 1.0
max_depth = 40 = 40
val = SimpleSolver() = Multilane.SimpleSolver()
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:11:54
reward:  0.296 ±  0.009
k = "pftdpw_5"
Simulating...100%|██████████████████████████████████████| Time: 0:14:18
reward:  0.207 ±  0.008
k = "pftdpw_10"
Simulating...100%|██████████████████████████████████████| Time: 0:14:32
reward:  0.203 ±  0.009
k = "pftdpw_20"
Simulating...100%|██████████████████████████████████████| Time: 0:14:31
reward:  0.211 ±  0.009
k = "simple"
Simulating...100%|██████████████████████████████████████| Time: 0:04:27
reward:  0.089 ±  0.008
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:11:37
reward:  0.309 ±  0.009
k = "pftdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:14:31
reward:  0.214 ±  0.009
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:10:42
reward:  0.360 ±  0.008
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/multilane_Monday_30_Apr_23_41.csv...
done.


Mon 30 Apr 2018 09:53:51 PM PDT

for some reason it didn't save the journal, but in the below results, pomcpow was 0.260 and despot was .289
the csv is there somewhere

Mon 30 Apr 2018 07:31:47 PM PDT

looks like m=10 is indeed the best for pftdpw
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:13:16
reward:  0.258 ±  0.008
k = "pftdpw_5"
Simulating...100%|██████████████████████████████████████| Time: 0:15:21
reward:  0.174 ±  0.008
k = "pftdpw_20"
Simulating...100%|██████████████████████████████████████| Time: 0:14:48
reward:  0.189 ±  0.009
k = "pftdpw_15"
Simulating...100%|██████████████████████████████████████| Time: 0:14:45
reward:  0.206 ±  0.008
k = "pftdpw_50"
Simulating...100%|██████████████████████████████████████| Time: 0:20:16
reward:  0.028 ±  0.008

Mon 30 Apr 2018 08:19:37 PM PDT

zsunberg@bethpage:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 54 multilane/compare.jl
cor = 0.75 = 0.75
lambda = 2.0 = 2.0
N = 1000 = 1000
n_iters = 1000000 = 1000000
max_time = 1.0 = 1.0
max_depth = 40 = 40
val = SimpleSolver() = Multilane.SimpleSolver()
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:13:16
reward:  0.258 ±  0.008
k = "pftdpw_5"
Simulating...100%|██████████████████████████████████████| Time: 0:15:21
reward:  0.174 ±  0.008
k = "pftdpw_20"
Simulating...100%|██████████████████████████████████████| Time: 0:14:48
reward:  0.189 ±  0.009
k = "pftdpw_15"
Simulating...100%|██████████████████████████████████████| Time: 0:14:45
reward:  0.206 ±  0.008
k = "pftdpw_50"
Simulating...100%|██████████████████████████████████████| Time: 0:20:16
reward:  0.028 ±  0.008
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:12:55
reward:  0.260 ±  0.008
k = "simple"
Simulating...100%|██████████████████████████████████████| Time: 0:05:04
reward:  0.047 ±  0.008
k = "pftdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:14:46
reward:  0.199 ±  0.008
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:12:23
reward:  0.289 ±  0.008
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/multilane_Monday_30_Apr_20
_16.csv...
done.

dangit - of course despot outperforms

Mon 30 Apr 2018 03:53:00 PM PDT

multilane comparison
data/multilane_Saturday_28_Apr_16_17.csv

Tue 06 Mar 2018 12:39:10 PM PST

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018$ julia -p 54 run_all.jl

LASERTAG

max_time = 1.0 = 1.0
max_depth = 90 = 90
exploration = 26.0 = 26.0
N = 1000 = 1000
k = "qmdp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:20
Simulating...100%|██████████████████████████████████████| Time: 0:01:27
reward: -10.545 ±  0.195
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:11:34
reward: -10.324 ±  0.166
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:13:10
reward: -10.638 ±  0.189
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:12
Simulating...100%|██████████████████████████████████████| Time: 0:10:18
reward: -11.073 ±  0.161
k = "d_pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:25:13
reward: -14.124 ±  0.210
k = "despot"
Creating Simulations...100%|████████████████████████████| Time: 0:07:10
Simulating...100%|██████████████████████████████████████| Time: 0:13:12
reward: -8.902 ±  0.186
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Monday
_5_Mar_20_17.csv...
done.

SIMPLELD

max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 24.876 ±  0.861
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:09
reward: -6.369 ±  1.029
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:30:01
reward: -7.263 ±  1.000
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:03:43
reward: 56.114 ±  0.559
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:03:06
reward: 61.059 ±  0.396
k = "heuristic_1"
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 27.612 ±  0.917
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:05:21
reward: -6.819 ±  1.020
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:27:20
reward: -7.102 ±  1.004
k = "side"
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 42.417 ±  0.432
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:17
reward: 57.157 ±  0.468
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:08:17
reward: 54.167 ±  1.058
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/simpleld_Monday
_5_Mar_21_39.csv...
done.

SUBHUNT

[Iteration 1   ] residual:        100 | iteration runtime:    569.411 ms, (     0.569 s total)
[Iteration 2   ] residual:         99 | iteration runtime:    571.663 ms, (      1.14 s total)
[Iteration 3   ] residual:         98 | iteration runtime:    605.457 ms, (      1.75 s total)
[Iteration 4   ] residual:       95.5 | iteration runtime:    591.943 ms, (      2.34 s total)
[Iteration 5   ] residual:       90.8 | iteration runtime:    588.740 ms, (      2.93 s total)
[Iteration 6   ] residual:       88.9 | iteration runtime:    572.866 ms, (       3.5 s total)
[Iteration 7   ] residual:       84.4 | iteration runtime:    611.043 ms, (      4.11 s total)
[Iteration 8   ] residual:       77.8 | iteration runtime:    604.173 ms, (      4.72 s total)
[Iteration 9   ] residual:       73.7 | iteration runtime:    602.763 ms, (      5.32 s total)
[Iteration 10  ] residual:       35.2 | iteration runtime:    591.217 ms, (      5.91 s total)
[Iteration 11  ] residual:       32.7 | iteration runtime:    571.201 ms, (      6.48 s total)
[Iteration 12  ] residual:       25.8 | iteration runtime:    572.200 ms, (      7.05 s total)
[Iteration 13  ] residual:       6.78 | iteration runtime:    572.061 ms, (      7.62 s total)
[Iteration 14  ] residual:       1.69 | iteration runtime:    571.424 ms, (       8.2 s total)
[Iteration 15  ] residual:      0.272 | iteration runtime:    589.047 ms, (      8.79 s total)
[Iteration 16  ] residual:     0.0281 | iteration runtime:    572.826 ms, (      9.36 s total)
[Iteration 17  ] residual:     0.0018 | iteration runtime:    571.191 ms, (      9.93 s total)
[Iteration 18  ] residual:   7.91E-05 | iteration runtime:    575.093 ms, (      10.5 s total)
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:45
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:32
reward: 79.014 ±  1.081
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:23
reward: 69.222 ±  1.259
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:05:13
reward: 28.259 ±  1.347
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 77.361 ±  1.109
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:14
reward: 27.370 ±  1.333
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:26
reward: 27.981 ±  1.344
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 26.814 ±  1.325
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:26
reward: 28.245 ±  1.347
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Monday_
5_Mar_22_08.csv...
done.

VDPBARRIER

max_time = 1.0 = 1.0
max_depth = 10 = 10
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:00:10
reward: -2.733 ±  1.233
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:06:01
reward: 29.343 ±  0.825
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:00:13
reward: -17.055 ±  0.384
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:06:38
reward: 27.242 ±  0.839
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:12:53
reward: 16.409 ±  0.957
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:13:31
reward: 14.287 ±  0.951
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:13:08
reward: 14.669 ±  0.946
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/vdpbarrier_Mond
ay_5_Mar_23_01.csv...
done.

FILENAMES

simpleld => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/simpleld_Mond
ay_5_Mar_21_39.csv
subhunt => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Monday
_5_Mar_22_08.csv
lasertag => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Mond
ay_5_Mar_20_17.csv
vdpbarrier => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/vdpbarrier_
Monday_5_Mar_23_01.csv
filenames = Dict("simpleld"=>"/home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/
data/simpleld_Monday_5_Mar_21_39.csv","subhunt"=>"/home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExp
eriments/icaps_2018/data/subhunt_Monday_5_Mar_22_08.csv","lasertag"=>"/home/zsunberg/.julia/v0.6/Continuo
usPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Monday_5_Mar_20_17.csv","vdpbarrier"=>"/home/zsunbe
rg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/vdpbarrier_Monday_5_Mar_23_01.csv")


Thu 01 Mar 2018 11:07:39 AM PST

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/vdpbarrier_table.jl
max_time = 1.0 = 1.0
max_depth = 10 = 10
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:00:15
reward: -2.733 ±  1.233
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:06:40
reward: 29.674 ±  0.836
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:00:14
reward: -16.288 ±  0.393
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:07:11
reward: 28.178 ±  0.858
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:13:16
reward: 17.073 ±  0.950
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:14:40
reward: 13.810 ±  0.950
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:14:10
reward: 12.733 ±  0.914
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/vdpbarrier_Thursday_1_Mar_19_36.csv...

Thu 01 Mar 2018 09:04:43 AM PST

pft_opt with mean reward bmdp

Simulating...100%|██████████████████████████████████████| Time: 0:20:09
mean(combined[:mean_reward]) = 27.027060837816983
iteration 32
mean(d) = [19.3555, 69.612, 22.2074, 25.485, 7.75552, 84.81]
det(cov(d)) = 0.06679763750793188
ev = eigvals(cov(d)) = [0.0124856, 0.0190092, 0.170673, 0.743723, 9.79803, 226.293]
(eigvecs(cov(d)))[:, j] = [0.842685, -0.0518326, 0.433574, -0.204976, -0.23907, 0.00620807]
(eigvecs(cov(d)))[:, j] = [-0.320754, -0.151279, -0.00913584, -0.251411, -0.899775, -0.036677]
(eigvecs(cov(d)))[:, j] = [-0.418459, 0.0591947, 0.879368, -0.0573225, 0.152303, -0.147029]
(eigvecs(cov(d)))[:, j] = [-0.0684682, -0.421876, -0.126099, -0.829423, 0.33094, -0.0629885]
(eigvecs(cov(d)))[:, j] = [-0.0758077, 0.491786, 0.0457648, -0.312836, -0.00163842, 0.807735]
(eigvecs(cov(d)))[:, j] = [0.0381511, 0.742355, -0.143687, -0.325087, -0.0230388, -0.566211]

Tue 27 Feb 2018 11:02:59 PM PST

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018$ julia -p 51 run_all.jl

LASERTAG

max_time = 1.0 = 1.0
max_depth = 90 = 90
exploration = 26.0 = 26.0
N = 1000 = 1000
k = "qmdp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:20
Simulating...100%|██████████████████████████████████████| Time: 0:01:37
reward: -10.545 ±  0.195
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:12:51
reward: -10.275 ±  0.171
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:14:17
reward: -10.657 ±  0.191
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:12
Simulating...100%|██████████████████████████████████████| Time: 0:10:36
reward: -10.576 ±  0.162
k = "despot"
Creating Simulations...100%|████████████████████████████| Time: 0:08:09
Simulating...100%|██████████████████████████████████████| Time: 0:14:30
reward: -8.983 ±  0.180
k = "pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:29
Simulating...100%|██████████████████████████████████████| Time: 0:26:04
reward: -14.226 ±  0.211
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Monday
_26_Feb_18_46.csv...
done.

SIMPLELD

max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:03
reward: 24.876 ±  0.861
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:32:21
reward: -6.940 ±  1.006
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:03:38
reward: 57.691 ±  0.491
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 61.032 ±  0.395
k = "heuristic_1"
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 27.612 ±  0.917
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:05:29
reward: -6.819 ±  1.020
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:28:59
reward: -6.874 ±  1.009
k = "side"
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 42.417 ±  0.432
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:27
reward: 57.086 ±  0.450
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:09:18
reward: 54.609 ±  1.029
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/simpleld_Monday
_26_Feb_20_13.csv...
done.

SUBHUNT

[Iteration 1   ] residual:        100 | iteration runtime:    562.230 ms, (     0.562 s total)
[Iteration 2   ] residual:         99 | iteration runtime:    564.119 ms, (      1.13 s total)
[Iteration 3   ] residual:         98 | iteration runtime:    562.861 ms, (      1.69 s total)
[Iteration 4   ] residual:       95.5 | iteration runtime:    596.654 ms, (      2.29 s total)
[Iteration 5   ] residual:       90.8 | iteration runtime:    672.064 ms, (      2.96 s total)
[Iteration 6   ] residual:       88.9 | iteration runtime:    633.517 ms, (      3.59 s total)
[Iteration 7   ] residual:       84.4 | iteration runtime:    608.742 ms, (       4.2 s total)
[Iteration 8   ] residual:       77.8 | iteration runtime:    605.443 ms, (      4.81 s total)
[Iteration 9   ] residual:       73.7 | iteration runtime:    564.188 ms, (      5.37 s total)
[Iteration 10  ] residual:       35.2 | iteration runtime:    596.072 ms, (      5.97 s total)
[Iteration 11  ] residual:       32.7 | iteration runtime:    612.366 ms, (      6.58 s total)
[Iteration 12  ] residual:       25.8 | iteration runtime:    612.047 ms, (      7.19 s total)
[Iteration 13  ] residual:       6.78 | iteration runtime:    607.669 ms, (       7.8 s total)
[Iteration 14  ] residual:       1.69 | iteration runtime:    560.812 ms, (      8.36 s total)
[Iteration 15  ] residual:      0.272 | iteration runtime:    627.969 ms, (      8.99 s total)
[Iteration 16  ] residual:     0.0281 | iteration runtime:    614.489 ms, (       9.6 s total)
[Iteration 17  ] residual:     0.0018 | iteration runtime:    618.567 ms, (      10.2 s total)
[Iteration 18  ] residual:   7.91E-05 | iteration runtime:    615.621 ms, (      10.8 s total)
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:47
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:32
reward: 79.014 ±  1.081
k = "ar_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:22
reward: 27.002 ±  1.328
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:32
reward: 69.221 ±  1.259
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:05:27
reward: 27.614 ±  1.339
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:29
reward: 78.683 ±  1.056
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:23
reward: 27.370 ±  1.333
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 27.814 ±  1.342
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 28.786 ±  1.353
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Monday_
26_Feb_20_44.csv...
done.

VDPBARRIER

max_time = 1.0 = 1.0
max_depth = 10 = 10
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:00:17
reward: -7.969 ±  1.331
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:06:42
reward: 30.573 ±  0.829
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:00:25
reward: -17.279 ±  0.317
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:08:12
reward: 23.208 ±  0.896
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:13:46
reward: 16.035 ±  0.966
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:14:49
reward: 12.789 ±  0.926
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:13:42
reward: 16.108 ±  0.984
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/bdpbarrier_Mond
ay_26_Feb_21_42.csv...
done.

SUBHUNT_DISCRETIZATION.JL

max_depth = 20 = 20
max_time = 1.0 = 1.0
N = 1000 = 1000
max_time = 1.0
(k, binsize) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 69.147 ±  1.258
(k, binsize) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:20
reward: 28.926 ±  1.354
(k, binsize) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 27.986 ±  1.344
(k, binsize) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 69.297 ±  1.254
(k, binsize) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:21
reward: 26.500 ±  1.322
(k, binsize) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 28.334 ±  1.348
(k, binsize) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 68.457 ±  1.275
(k, binsize) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:21
reward: 26.690 ±  1.325
(k, binsize) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 28.067 ±  1.344
(k, binsize) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:39
reward: 67.440 ±  1.289
(k, binsize) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:22
reward: 25.947 ±  1.314
(k, binsize) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 27.978 ±  1.343
(k, binsize) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:40
reward: 66.194 ±  1.310
(k, binsize) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:22
reward: 27.184 ±  1.330
(k, binsize) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 28.521 ±  1.350
(k, binsize) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 68.064 ±  1.278
(k, binsize) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:03:25
reward: 26.223 ±  1.318
(k, binsize) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 28.174 ±  1.346
(k, binsize) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:05:00
reward: 44.517 ±  1.455
(k, binsize) = ("d_despot", 10.0)
Simulating... 92%|███████████████████████████████████   |  ETA: 0:00:19^[[B
Simulating...100%|██████████████████████████████████████| Time: 12:20:46
reward: 26.903 ±  1.329
(k, binsize) = ("d_pomcp", 10.0)
Simulating... 30%|████████████                          |  ETA: 0:03:21
Simulating...100%|██████████████████████████████████████| Time: 0:05:37
reward: 28.562 ±  1.355
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_discret
ization_Tuesday_27_Feb_11_38.csv...
done.

LD_DISCRETIZATION.JL

max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
(k, d) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:15
reward: 59.594 ±  0.409
(k, d) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:41
reward: 57.645 ±  0.445
(k, d) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:08:10
reward: -28.822 ±  2.424
(k, d) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:27
reward: 59.297 ±  0.414
(k, d) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:07
reward: 60.991 ±  0.412
(k, d) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:08:07
reward: -12.637 ±  2.401
(k, d) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:33
reward: 58.598 ±  0.427
(k, d) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:05:11
reward: 53.900 ±  0.771
(k, d) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:08:53
reward:  2.588 ±  2.252
(k, d) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:36
reward: 58.141 ±  0.428
(k, d) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:28:46
reward: -9.846 ±  0.824
(k, d) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:08:34
reward: 14.237 ±  2.115
(k, d) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:07
reward: 53.684 ±  0.515
(k, d) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:30:29
reward: -11.405 ±  0.838
(k, d) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:08:32
reward: 53.812 ±  1.071
(k, d) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:20
reward: 52.095 ±  0.579
(k, d) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:30:51
reward: -13.575 ±  0.707
(k, d) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:08:56
reward: 54.556 ±  1.112
(k, d) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:06:18
reward: 40.621 ±  0.911
(k, d) = ("d_pomcp", 10.0)
Simulating... 32%|████████████                          |  ETA: 0:22:05
Simulating...100%|██████████████████████████████████████| Time: 0:32:13
reward: -16.725 ±  0.536
(k, d) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:08:18
reward: 39.906 ±  1.656
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/ld_discretizati
on_Tuesday_27_Feb_15_23.csv...
done.

FILENAMES

ld_discretization.jl => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/l
d_discretization_Tuesday_27_Feb_15_23.csv
simpleld => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/simpleld_Mond
ay_26_Feb_20_13.csv
subhunt => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Monday
_26_Feb_20_44.csv
lasertag => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Mond
ay_26_Feb_18_46.csv
subhunt_discretization.jl => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/d
ata/subhunt_discretization_Tuesday_27_Feb_11_38.csv
vdpbarrier => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/bdpbarrier_
Monday_26_Feb_21_42.csv
filenames = Dict("ld_discretization.jl"=>"/home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments
/icaps_2018/data/ld_discretization_Tuesday_27_Feb_15_23.csv","simpleld"=>"/home/zsunberg/.julia/v0.6/Cont
inuousPOMDPTreeSearchExperiments/icaps_2018/data/simpleld_Monday_26_Feb_20_13.csv","subhunt"=>"/home/zsun
berg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Monday_26_Feb_20_44.csv","l
asertag"=>"/home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Monda
y_26_Feb_18_46.csv","subhunt_discretization.jl"=>"/home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExp
eriments/icaps_2018/data/subhunt_discretization_Tuesday_27_Feb_11_38.csv","vdpbarrier"=>"/home/zsunberg/.
julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/bdpbarrier_Monday_26_Feb_21_42.csv")

Mon 26 Feb 2018 09:47:36 AM PST

pow_opt after barrier fix

iteration 16
mean(d) = [120.406, 28.0466, 30.7495, 5.46869, 114.904]
det(cov(d)) = 1.061947260056586e6
ev = eigvals(cov(d)) = [0.431383, 4.18512, 9.4225, 136.406, 457.648]
(eigvecs(cov(d)))[:, j] = [-0.0766408, -0.176956, -0.120116, 0.969829, 0.0884104]
(eigvecs(cov(d)))[:, j] = [0.345486, 0.86682, 0.242012, 0.227918, -0.136915]
(eigvecs(cov(d)))[:, j] = [-0.231256, 0.394253, -0.82353, -0.0781248, 0.326773]
(eigvecs(cov(d)))[:, j] = [0.0616476, 0.02729, 0.366556, -0.0293004, 0.927488]
(eigvecs(cov(d)))[:, j] = [0.904149, -0.247244, -0.338286, -0.0228664, 0.0801513]
creating 150 simulation sets......................................................................................................................................................
Simulating...100%|██████████████████████████████████████| Time: 0:20:18
mean(combined[:mean_reward]) = 28.22687967793777

Mon 26 Feb 2018 12:15:25 AM PST

pft_opt after barrier fix

Simulating...100%|██████████████████████████████████████| Time: 0:23:25
mean(combined[:mean_reward]) = 22.285554134050294
iteration 64
mean(d) = [14.0333, 86.7138, 18.553, 21.4697, 8.23664, 64.7801]
det(cov(d)) = 3.642743280683159e-11
ev = eigvals(cov(d)) = [7.02895e-6, 5.05806e-5, 0.00484179, 0.684519, 0.985454, 31.3709]
(eigvecs(cov(d)))[:, j] = [0.715766, 0.0888766, 0.393591, 0.112399, 0.554153, -0.0717465]
(eigvecs(cov(d)))[:, j] = [-0.605319, -0.00852499, 0.00964775, -0.0374598, 0.792353, 0.0647773]
(eigvecs(cov(d)))[:, j] = [0.324778, -0.152769, -0.897078, -0.0392837, 0.253444, 0.0256003]
(eigvecs(cov(d)))[:, j] = [0.0453787, 0.360066, -0.00923739, -0.871419, 0.0243529, -0.329004]
(eigvecs(cov(d)))[:, j] = [-0.062576, 0.892638, -0.197159, 0.394053, -0.011388, -0.0707365]
(eigvecs(cov(d)))[:, j] = [0.0990585, 0.205526, 0.0358774, -0.264142, -0.0115871, 0.936351]

Sat 24 Feb 2018 11:00:46 PM PST

After barrier fix

max_time = 1.0 = 1.0
max_depth = 10 = 10
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:02:20
reward: -6.606 ±  1.328
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:08:12
reward: 29.205 ±  0.855
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:03:55
reward: -17.460 ±  0.283
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:09:43
reward: 23.968 ±  0.918
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:14:46
reward: 15.003 ±  0.961
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:16:22
reward: 10.161 ±  0.904
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:14:35
reward: 14.884 ±  0.960

Sat 24 Feb 2018 09:13:31 PM PST

pft_opt for Random vdp tag before barrier fix

mean(combined[:mean_reward]) = 19.881941923853947
iteration 16
mean(d) = [13.7315, 75.8478, 21.2246, 21.1542, 7.64275, 63.8986]
det(cov(d)) = 138465.70897763397
ev = eigvals(cov(d)) = [0.174967, 1.61616, 3.06432, 20.7234, 62.6792, 123.023]
(eigvecs(cov(d)))[:, j] = [0.149519, 0.0428675, 0.138351, -0.135943, 0.968548, 0.0100085]
(eigvecs(cov(d)))[:, j] = [-0.883995, -0.0257854, -0.381891, 0.153053, 0.213053, 0.0568334]
(eigvecs(cov(d)))[:, j] = [-0.44218, 0.053969, 0.792449, -0.371542, -0.0978066, -0.161181]
(eigvecs(cov(d)))[:, j] = [-0.00153459, -0.361093, 0.356489, 0.81028, 0.0819346, -0.281527]
(eigvecs(cov(d)))[:, j] = [-0.0250064, -0.0159634, 0.271169, 0.201888, -0.0155503, 0.940525]
(eigvecs(cov(d)))[:, j] = [0.00677043, -0.929485, -0.0801613, -0.35034, 0.00151613, 0.0827431]

Sat 24 Feb 2018 02:07:48 PM PST

pow_opt for Random vdp tag

Simulating...100%|██████████████████████████████████████| Time: 0:21:48
mean(combined[:mean_reward]) = 25.774977146997575
iteration 48
mean(d) = [86.898, 26.0576, 28.4142, 6.85923, 105.764]
det(cov(d)) = 2.3484144732571604e-5
ev = eigvals(cov(d)) = [0.000163307, 0.00109723, 0.157391, 10.6971, 77.8439]
(eigvecs(cov(d)))[:, j] = [-0.101314, -0.00720456, 0.612271, 0.782309, -0.0529097]
(eigvecs(cov(d)))[:, j] = [0.133097, -0.34949, -0.721847, 0.581291, 0.0343396]
(eigvecs(cov(d)))[:, j] = [-0.322572, -0.879036, 0.212012, -0.202759, 0.192828]
(eigvecs(cov(d)))[:, j] = [-0.931578, 0.257634, -0.243062, 0.0689728, -0.0441557]
(eigvecs(cov(d)))[:, j] = [0.0113835, 0.196787, 0.00569271, 0.0649899, 0.978207]

Fri 23 Feb 2018 07:08:26 PM PST

max_time = 1.0 = 1.0
max_depth = 10 = 10
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:02:55
reward: -21.496 ±  1.089
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:08:55
reward: 27.340 ±  0.807
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:04:04
reward: -18.300 ±  0.240
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:10:36
reward: 21.398 ±  0.905
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:14:32
reward: 15.805 ±  0.914
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:17:32
reward:  8.548 ±  0.862
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:15:56
reward: 12.237 ±  0.877

Fri 23 Feb 2018 12:35:24 PM PST

With barriers (!!) random rollouts

max_time = 1.0 = 1.0
max_depth = 10 = 10
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:02:56
reward: -21.496 ±  1.089
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:08:50
reward: 27.484 ±  0.820
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:11:55
reward: 20.522 ±  0.905

With barriers, tonext rollouts
RO = ToNextMLSolver = VDPTag2.ToNextMLSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:02:55
reward: -21.496 ±  1.089
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:16:20
reward: 17.485 ±  0.987
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:12:31
reward: 17.525 ±  0.956


Thu 22 Feb 2018 10:58:34 PM PST

mean(combined[:mean_reward]) = 35.92344192575698
iteration 16
mean(d) = [100.335, 25.5375, 19.8407, 5.45474, 107.777]
det(cov(d)) = 1703.1403575426195
ev = eigvals(cov(d)) = [0.119598, 0.404508, 2.79202, 91.3478, 138.034]
(eigvecs(cov(d)))[:, j] = [0.0705534, 0.655333, -0.258949, -0.66055, 0.249359]
(eigvecs(cov(d)))[:, j] = [0.115753, 0.589482, -0.214097, 0.748976, 0.17975]
(eigvecs(cov(d)))[:, j] = [-0.128394, -0.326238, -0.935792, 0.0170253, -0.0329793]
(eigvecs(cov(d)))[:, j] = [-0.978743, 0.184658, 0.071931, 0.0358061, -0.0388228]
(eigvecs(cov(d)))[:, j] = [-0.0848558, -0.287263, 0.0789144, 0.0337159, 0.950219]

Thu 22 Feb 2018 04:03:52 PM PST

!! With sampled rollouts
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:06:54
reward: 35.952 ±  0.756
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:08:46
reward: 29.060 ±  0.871


Wed 21 Feb 2018 06:02:02 PM PST

PFT opt with single sample rollouts (uh oh)

Simulating...100%|██████████████████████████████████████| Time: 0:19:57
mean(combined[:mean_reward]) = 29.854269742495255
iteration 43
mean(d) = [15.2746, 86.8576, 21.0007, 20.7577, 5.59402, 54.8343]
det(cov(d)) = 2.1345807713889652e-6
ev = eigvals(cov(d)) = [0.00123411, 0.00433508, 0.0496962, 0.257932, 1.3166, 23.6417]
(eigvecs(cov(d)))[:, j] = [0.75881, 0.0841258, -0.227082, 0.218579, -0.517278, -0.224077]
(eigvecs(cov(d)))[:, j] = [0.578282, 0.295266, 0.224886, -0.0956049, 0.712911, 0.102233]
(eigvecs(cov(d)))[:, j] = [0.0420975, 0.418438, 0.28644, -0.527804, -0.442631, 0.516323]
(eigvecs(cov(d)))[:, j] = [-0.0391215, 0.200461, -0.889522, -0.114156, 0.162792, 0.357075]
(eigvecs(cov(d)))[:, j] = [0.0722885, -0.192936, -0.154998, -0.797661, 0.0411566, -0.543662]
(eigvecs(cov(d)))[:, j] = [-0.285077, 0.808233, -0.0231928, 0.123331, -0.00799412, -0.499678]

Wed 21 Feb 2018 06:01:12 PM PST

PFT opt with full belief rollouts

mean(combined[:mean_reward]) = 24.406039463499653
iteration 16
mean(d) = [10.0884, 64.2481, 16.4526, 13.9518, 4.77987, 58.2047]
det(cov(d)) = 221.66408916796289
ev = eigvals(cov(d)) = [0.0985605, 0.477854, 0.728103, 2.75178, 41.7087, 56.3202]
(eigvecs(cov(d)))[:, j] = [-0.200634, -0.0551381, -0.315826, 0.0957057, -0.920166, -0.0330813]
(eigvecs(cov(d)))[:, j] = [-0.643877, -0.114834, 0.213639, 0.702378, 0.150693, -0.102722]
(eigvecs(cov(d)))[:, j] = [0.0293745, -0.101649, 0.915678, -0.19415, -0.335313, 0.0144812]
(eigvecs(cov(d)))[:, j] = [0.70431, 0.0218577, 0.0774332, 0.675213, -0.117224, 0.166794]
(eigvecs(cov(d)))[:, j] = [0.178615, -0.873455, -0.0822278, -0.0302184, 0.0543308, -0.441075]
(eigvecs(cov(d)))[:, j] = [-0.127867, -0.458301, -0.0582189, -0.0546492, 0.0381803, 0.875087]
creating 120 simulation sets........................................................................................................................

Tue 20 Feb 2018 10:14:41 AM PST

Simulating...100%|██████████████████████████████████████| Time: 0:36:48
mean(combined[:mean_reward]) = 44.49592403115041
iteration 16
mean(d) = [11.0231, 72.3433, 25.3049, 16.8478, 27.5656, 1.58413]
det(cov(d)) = 37.838496819420826
ev = eigvals(cov(d)) = [0.00731722, 0.807921, 1.68545, 3.97169, 7.97678, 119.867]
(eigvecs(cov(d)))[:, j] = [-0.00243827, -0.00628171, -0.0748551, -0.0522666, 0.055383, 0.99426]
(eigvecs(cov(d)))[:, j] = [-0.166587, -0.0497107, 0.88766, -0.206226, -0.365498, 0.0756252]
(eigvecs(cov(d)))[:, j] = [0.283243, -0.223624, 0.178723, -0.618369, 0.672435, -0.0572258]
(eigvecs(cov(d)))[:, j] = [-0.463711, -0.548425, 0.17972, 0.511272, 0.436317, 0.0115012]
(eigvecs(cov(d)))[:, j] = [-0.822642, 0.227429, -0.225106, -0.465093, 0.0504669, -0.0447885]
(eigvecs(cov(d)))[:, j] = [0.0157834, -0.771355, -0.30256, -0.30765, -0.467179, -0.0177631]

Mon 19 Feb 2018 11:11:46 PM PST

pow opt after agent_speed = 1.5

Simulating...100%|██████████████████████████████████████| Time: 0:44:46
mean(combined[:mean_reward]) = 45.81972818279902
iteration 13
mean(d) = [21.7926, 35.0444, 14.7549, 3.33754, 97.168]
det(cov(d)) = 470.677844296718
ev = eigvals(cov(d)) = [0.0171842, 1.27947, 3.76389, 38.0314, 149.549]
(eigvecs(cov(d)))[:, j] = [0.0485464, -0.106695, 0.241261, 0.962812, -0.0323506]
(eigvecs(cov(d)))[:, j] = [0.475388, 0.304957, -0.786606, 0.211386, 0.132569]
(eigvecs(cov(d)))[:, j] = [0.78214, -0.541715, 0.18982, -0.153341, -0.187767]
(eigvecs(cov(d)))[:, j] = [0.353314, 0.773985, 0.455753, -0.0548401, -0.255731]
(eigvecs(cov(d)))[:, j] = [0.187286, 0.0557678, 0.281604, -0.0422948, 0.938469]

Mon 19 Feb 2018 10:01:58 AM PST

POMCPOW does not outperform PFT on any problems

k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:02:04
reward: 14.228 ±  1.621
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:06:26
reward: 37.260 ±  0.871
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:03:44
reward: -16.254 ±  0.412
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:05:08
reward: 36.881 ±  0.891
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:30:15
reward: -13.277 ±  0.443
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:34:39
reward: -13.120 ±  0.549
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:40:12
reward: -18.445 ±  0.257

Sat 17 Feb 2018 02:02:37 PM PST

pow opt for vdp tag after pow fix

Simulating...100%|██████████████████████████████████████| Time: 0:54:05
mean(combined[:mean_reward]) = 37.76751921884238
iteration 42
mean(d) = [31.5108, 25.9038, 8.38779, 4.96295, 96.9681]
det(cov(d)) = 1.0596841743602374e-6
ev = eigvals(cov(d)) = [2.97035e-5, 0.0614394, 0.224651, 0.609569, 4.24025]
(eigvecs(cov(d)))[:, j] = [0.0365483, -0.259587, 0.548476, 0.793156, -0.0368381]
(eigvecs(cov(d)))[:, j] = [-0.135531, 0.469818, -0.632458, 0.599298, 0.0417313]
(eigvecs(cov(d)))[:, j] = [-0.175859, 0.235181, 0.238477, -0.0368755, 0.92495]
(eigvecs(cov(d)))[:, j] = [0.72372, -0.449493, -0.375303, 0.0954354, 0.352457]
(eigvecs(cov(d)))[:, j] = [0.652379, 0.67419, 0.31851, -0.0357439, -0.130932]


Wed 14 Feb 2018 01:33:13 PM PST

RESULT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/subhunt_discr
etization.jl
max_depth = 20 = 20
max_time = 1.0 = 1.0
N = 1000 = 1000
max_time = 1.0
(k, binsize) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:47
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:32
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 62.105 ±  1.361
(k, binsize) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:19
reward: 29.104 ±  1.356
(k, binsize) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.083 ±  1.345
(k, binsize) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:32
reward: 64.007 ±  1.333
(k, binsize) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:18
reward: 26.684 ±  1.325
(k, binsize) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.168 ±  1.346
(k, binsize) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:43
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:33
reward: 62.163 ±  1.360
(k, binsize) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:18
reward: 26.690 ±  1.325
(k, binsize) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.077 ±  1.345
(k, binsize) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:31
reward: 62.531 ±  1.355
(k, binsize) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:18
reward: 25.946 ±  1.314
(k, binsize) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:34
reward: 28.269 ±  1.348
(k, binsize) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:43
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:29
reward: 63.835 ±  1.339
(k, binsize) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:18
reward: 27.278 ±  1.332
(k, binsize) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 27.355 ±  1.336
(k, binsize) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:30
reward: 61.437 ±  1.371
(k, binsize) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:03:22
reward: 26.314 ±  1.319
(k, binsize) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.632 ±  1.352
(k, binsize) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:43
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:56
reward: 39.193 ±  1.440
(k, binsize) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:42
reward: 26.627 ±  1.325
(k, binsize) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:31
reward: 28.576 ±  1.356
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_discretization_Wednesday_14_Feb_13_32.csv...
done.

Wed 14 Feb 2018 11:46:27 AM PST

RESULT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/simpleld_table.jl
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:06
reward: 24.876 ±  0.861
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:10
reward: -6.369 ±  1.029
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:32:23
reward: -7.269 ±  0.997
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:03:38
reward: 57.624 ±  0.475
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:03:03
reward: 61.182 ±  0.435
k = "heuristic_1"
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 27.612 ±  0.917
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:05:21
reward: -6.819 ±  1.020
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:29:37
reward: -7.813 ±  0.991
k = "side"
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 42.417 ±  0.432
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:04:31
reward: 49.366 ±  0.690
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:08:17
reward: 55.540 ±  0.974

Wed 14 Feb 2018 09:46:58 AM PST

RESULT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/ld_discretiza
tion.jl
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
(k, d) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:14
reward: -6.369 ±  1.029
(k, d) = ("side", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:24
reward: 59.339 ±  0.421
(k, d) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:23
reward: 57.830 ±  0.445
(k, d) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:08:25
reward: -27.790 ±  2.433
(k, d) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:10
reward: -6.369 ±  1.029
(k, d) = ("side", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:28
reward: 59.069 ±  0.428
(k, d) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:05
reward: 61.135 ±  0.410
(k, d) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:08:04
reward: -12.014 ±  2.393
(k, d) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:10
reward: -6.369 ±  1.029
(k, d) = ("side", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:29
reward: 58.200 ±  0.471
(k, d) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:05:15
reward: 53.406 ±  0.789
(k, d) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:08:05
reward:  2.324 ±  2.253
(k, d) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:36
reward: 57.817 ±  0.448
(k, d) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:29:00
reward: -10.272 ±  0.806
(k, d) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:08:20
reward: 18.855 ±  2.058
(k, d) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:10
reward: -6.369 ±  1.029
(k, d) = ("side", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:08
reward: 53.712 ±  0.536
(k, d) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:30:49
reward: -12.893 ±  0.769
(k, d) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:08:14
reward: 55.927 ±  0.912
(k, d) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 50.634 ±  0.638
(k, d) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:30:32
reward: -12.937 ±  0.759
(k, d) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:08:14
reward: 53.911 ±  1.161
(k, d) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:10
reward: -6.369 ±  1.029
(k, d) = ("side", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:07:02
reward: 39.883 ±  0.928
(k, d) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:32:02
reward: -16.427 ±  0.555
(k, d) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:08:05
reward: 40.247 ±  1.637

Tue 13 Feb 2018 09:02:56 PM PST

RESULT subhunt time (nothing really interesting)

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/subhunt_time.
jl
max_depth = 20 = 20
N = 1000 = 1000
max_time = 0.01
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:47
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:00:36
reward: 14.869 ±  1.070
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:00:55
reward: 32.375 ±  1.387
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 0:00:34
reward: 36.367 ±  1.413
max_time = 0.03162277660168379
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:00:40
reward: 42.949 ±  1.444
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:00:59
reward: 36.877 ±  1.424
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 0:00:38
reward: 58.069 ±  1.388
max_time = 0.1
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:00:53
reward: 62.326 ±  1.360
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:01:19
reward: 44.206 ±  1.451
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 0:00:54
reward: 61.202 ±  1.363
max_time = 0.31622776601683794
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:01:31
reward: 67.596 ±  1.286
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:02:18
reward: 52.240 ±  1.436
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 0:01:37
reward: 66.222 ±  1.301
max_time = 1.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:03:38
reward: 72.706 ±  1.202
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:33
reward: 61.715 ±  1.365
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 1:02:39
reward: 71.370 ±  1.215
max_time = 3.1622776601683795
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:10:23
reward: 75.057 ±  1.156
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:11:16
reward: 74.319 ±  1.155
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 0:10:49
reward: 73.467 ±  1.178
max_time = 10.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:31:13
reward: 75.995 ±  1.140
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:33:39
reward: 74.901 ±  1.142
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 0:33:09
reward: 74.041 ±  1.173
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_time_Tuesday_13_Feb_14_03.csv...
done.

Tue 13 Feb 2018 09:00:06 PM PST

SWITCHED TO NEW LD Definition to prevent good side strategy

Mon 12 Feb 2018 07:20:31 PM PST

RESULT

k = "qmdp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:20
Simulating...100%|██████████████████████████████████████| Time: 0:01:35
reward: -10.545 ±  0.195
k = "ar_despot"
Creating Simulations...100%|████████████████████████████| Time: 0:07:46
Simulating...100%|██████████████████████████████████████| Time: 0:14:20
reward: -8.745 ±  0.180
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:19
Simulating...100%|██████████████████████████████████████| Time: 0:11:33
reward: -9.951 ±  0.176
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:14:14
reward: -10.746 ±  0.189
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:12
Simulating...100%|██████████████████████████████████████| Time: 0:11:46
reward: -11.910 ±  0.161
k = "pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:26:17
reward: -14.276 ±  0.211
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Monday_12_Feb_18_59.csv...

Mon 12 Feb 2018 10:39:56 AM PST

RESULT

zsunberg@tula:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 38 icaps_2018/vdptag2_table.jl 
max_time = 1.0 = 1.0
max_depth = 10 = 10
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:03:30
reward: 14.228 ±  1.621
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:08:33
reward: 38.650 ±  0.848
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:06:35
reward: -15.811 ±  0.438
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:06:53
reward: 34.617 ±  0.870
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:41:00
reward: -11.674 ±  0.516
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:46:20
reward: -13.107 ±  0.549
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:54:34
reward: -18.505 ±  0.265


Sat 10 Feb 2018 01:05:37 PM PST

pft opt for vdptag2

mean(combined[:mean_reward]) = 34.811950775101714
iteration 39
mean(d) = [8.64351, 17.79, 12.6579, 15.6675, 6.73998]
det(cov(d)) = 1.759567322143827e-8
ev = eigvals(cov(d)) = [0.000120243, 0.0171848, 0.0604909, 0.121802, 1.15574]
(eigvecs(cov(d)))[:, j] = [-0.38064, -0.583394, 0.611271, -0.343526, -0.151995]
(eigvecs(cov(d)))[:, j] = [0.878822, -0.441938, 0.0182528, -0.175703, -0.0340483]
(eigvecs(cov(d)))[:, j] = [0.0417059, -0.228413, 0.347697, 0.70925, 0.567591]
(eigvecs(cov(d)))[:, j] = [-0.24289, -0.435707, -0.589299, -0.289784, 0.565611]
(eigvecs(cov(d)))[:, j] = [0.148496, 0.471518, 0.397302, -0.513914, 0.577636]

Tue 30 Jan 2018 12:55:25 PM PST

pow_opt for vdptag2

mean(combined[:mean_reward]) = 39.34430949569718
iteration 21
mean(d) = [63.4931, 20.5716, 10.2502, 3.47125, 81.3509]
det(cov(d)) = 1.3200977801784242
ev = eigvals(cov(d)) = [0.016337, 0.0239187, 1.29561, 14.828, 175.848]
(eigvecs(cov(d)))[:, j] = [0.0070858, 0.184705, -0.959766, 0.191787, -0.0888853]
(eigvecs(cov(d)))[:, j] = [0.0813041, 0.0143906, 0.197992, 0.976702, 0.00592561]
(eigvecs(cov(d)))[:, j] = [-0.0113952, -0.982572, -0.177943, 0.0515584, -0.0100712]
(eigvecs(cov(d)))[:, j] = [0.780962, -0.0147681, 0.0461558, -0.0703953, -0.618703]
(eigvecs(cov(d)))[:, j] = [0.619118, -0.00345987, -0.0765128, -0.0407115, 0.780493]

Sat 27 Jan 2018 04:45:16 PM PST

RESULT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/subhunt_table
.jl
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:49
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
k = "ar_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:21
reward: 27.092 ±  1.329
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:33
reward: 61.809 ±  1.367
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:05:16
reward: 27.801 ±  1.341
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:38
reward: 72.675 ±  1.205
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:19
reward: 27.185 ±  1.331
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 27.786 ±  1.340
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:36
reward: 27.980 ±  1.343
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Saturday_27_Jan_13_26.csv...
ERROR: LoadError: UndefVarError: CSV not defined


Fri 26 Jan 2018 04:45:55 PM PST

RESULT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/subhunt_discretization.jl
max_depth = 20 = 20
max_time = 1.0 = 1.0
N = 1000 = 1000
max_time = 1.0
(k, binsize) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:47
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:18
reward: 29.104 ±  1.356
(k, binsize) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.353 ±  1.349
(k, binsize) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:43
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 26.591 ±  1.324
(k, binsize) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.251 ±  1.347
(k, binsize) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 26.780 ±  1.327
(k, binsize) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:34
reward: 28.425 ±  1.349
(k, binsize) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:17
reward: 25.857 ±  1.313
(k, binsize) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.340 ±  1.348
(k, binsize) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 27.276 ±  1.332
(k, binsize) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 27.080 ±  1.332
(k, binsize) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:03:20
reward: 26.225 ±  1.318
(k, binsize) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:33
reward: 28.087 ±  1.345
(k, binsize) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:42
reward: 26.458 ±  1.323
(k, binsize) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:30
reward: 28.750 ±  1.358
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_time_Friday_26_Jan_14_39.csv...
done.


Thu 25 Jan 2018 01:40:53 PM PST

mean(combined[:mean_reward]) = 61.4267447713561
iteration 16
mean(d) = [17.0653, 5.7884, 47.5693]
det(cov(d)) = 1.0654999602917252
ev = eigvals(cov(d)) = [0.0127303, 0.665979, 125.677]
(eigvecs(cov(d)))[:, j] = [-0.116846, 0.991147, -0.063049]
(eigvecs(cov(d)))[:, j] = [0.985356, 0.123633, 0.117421]
(eigvecs(cov(d)))[:, j] = [-0.124177, 0.0484054, 0.991079]

Wed 24 Jan 2018 05:30:45 PM PST

Subhunt pow opt stopped

mean(combined[:mean_reward]) = 53.971569571948294
iteration 5
mean(d) = [23.4068, 6.07487, 36.9538]
det(cov(d)) = 29773.25249346697
ev = eigvals(cov(d)) = [1.92731, 65.9505, 234.237]
(eigvecs(cov(d)))[:, j] = [-0.0361602, -0.989378, 0.1408]
(eigvecs(cov(d)))[:, j] = [0.991381, -0.0532668, -0.119691]
(eigvecs(cov(d)))[:, j] = [0.125919, 0.135258, 0.982776]
creating 60 simulation sets............................................................

Wed 24 Jan 2018 10:46:06 AM PST

Subhunt pow opt

mean(combined[:mean_reward]) = 47.534730786740525
iteration 14
mean(d) = [93.601, 7.5144, 16.4266]
det(cov(d)) = 0.488007248072637
ev = eigvals(cov(d)) = [0.0248916, 0.239055, 82.0117]
(eigvecs(cov(d)))[:, j] = [-0.0478393, 0.908884, 0.414295]
(eigvecs(cov(d)))[:, j] = [-0.341551, -0.404653, 0.848292]
(eigvecs(cov(d)))[:, j] = [0.938645, -0.100921, 0.329789]

Tue 23 Jan 2018 06:04:10 PM PST

with N = 1000

zsunberg@tula:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 38 icaps_2018/simpleld_table.jl 
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:08
reward: 24.469 ±  0.854
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:05:19
reward:  6.659 ±  1.267
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:04
reward: 62.179 ±  0.511
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:34:07
reward:  5.297 ±  1.254
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 57.123 ±  0.396
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:03:42
reward: 64.496 ±  0.383
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:09:21
reward: 52.163 ±  1.346
k = "heuristic_1"
Simulating...100%|██████████████████████████████████████| Time: 0:00:03
reward: 28.216 ±  0.898
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:32:52
reward:  4.491 ±  1.238


Tue 23 Jan 2018 03:52:38 PM PST

RESULT: LD Discretization

zsunberg@tula:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 38 icaps_2018/ld_discretizatio
n.jl
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
(k, d) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:08
reward: 62.052 ±  0.496
(k, d) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:24
reward: 61.234 ±  0.561
(k, d) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:09:12
reward: -22.312 ±  2.513
(k, d) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:09
reward: 62.153 ±  0.438
(k, d) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:41
reward: 64.365 ±  0.419
(k, d) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:09:05
reward: -4.192 ±  2.451
(k, d) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:13
reward: 62.234 ±  0.433
(k, d) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:60
reward: 62.244 ±  0.573
(k, d) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:09:40
reward:  7.768 ±  2.297
(k, d) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:08
reward: 62.127 ±  0.504
(k, d) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:26:48
reward: 13.883 ±  1.289            
(k, d) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:09:29
reward: 23.042 ±  2.112
(k, d) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 58.972 ±  0.605
(k, d) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:11:47
reward: 26.626 ±  0.801
(k, d) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:09:20
reward: 51.608 ±  1.374
(k, d) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:13
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:51
reward: 57.205 ±  0.717
(k, d) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:21:46
reward: 23.825 ±  1.350
(k, d) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:09:28
reward: 51.933 ±  1.383
(k, d) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:06:19
reward: 51.525 ±  0.912
(k, d) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:51
reward: 60.444 ±  0.498
(k, d) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:09:28
reward: 42.864 ±  1.711



Tue 23 Jan 2018 10:45:59 AM PST

mean(combined[:mean_reward]) = 61.980279863198554
iteration 47
mean(d) = [91.4278, 4.73208, 14.3109]
det(cov(d)) = 5.48831984150069e-8
ev = eigvals(cov(d)) = [4.31236e-7, 0.0197767, 6.43531]
(eigvecs(cov(d)))[:, j] = [-0.0269423, 0.929454, -0.367953]
(eigvecs(cov(d)))[:, j] = [0.0695339, 0.368938, 0.926849]
(eigvecs(cov(d)))[:, j] = [0.997216, -0.000613751, -0.0745686]

Mon 22 Jan 2018 02:12:00 PM PST

pow_opt for simple light-dark

mean(combined[:mean_reward]) = 61.649340998087304
iteration 10
mean(d) = [115.716, 4.18574, 13.7728]
ev = eigvals(cov(d)) = [0.202858, 1.1569, 621.191]
(eigvecs(cov(d)))[:, j] = [0.00148127, 0.999641, -0.0267648]
(eigvecs(cov(d)))[:, j] = [0.046333, 0.0266675, 0.99857]
(eigvecs(cov(d)))[:, j] = [0.998925, -0.00271925, -0.0462768]

Thu 18 Jan 2018 03:21:14 PM PST

RESULT: GOOD LASERTAG

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/lasertag_table.jl 
max_time = 1.0 = 1.0
max_depth = 90 = 90
exploration = 26.0 = 26.0
N = 1000 = 1000
k = "ar_despot"
Creating Simulations...100%|████████████████████████████| Time: 0:07:29
Simulating...100%|██████████████████████████████████████| Time: 0:13:47
reward: -8.919 ±  0.188
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:28
Simulating...100%|██████████████████████████████████████| Time: 0:12:20
reward: -10.283 ±  0.182
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:14:20
reward: -10.428 ±  0.202
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:12
Simulating... 26%|██████████                            |  ETA: 0:08:44^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[Simulating...100%|██████████████████████████████████████| Time: 0:11:05
reward: -11.586 ±  0.166
k = "pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:27:05
reward: -14.497 ±  0.213
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Thursday_18_Jan_15_16.csv...
done.
zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/lasertag_table.jl 
max_time = 1.0 = 1.0
max_depth = 90 = 90
exploration = 26.0 = 26.0
N = 1000 = 1000
k = "qmdp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:20
Simulating...100%|██████████████████████████████████████| Time: 0:01:37
reward: -10.486 ±  0.204
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Thursday_18_Jan_15_20.csv...
done.

Thu 11 Jan 2018 10:16:27 AM PST

Discretization preview

zach@Theresa:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018$ julia -p 6 ld_discretization.jl 
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 100 = 100
(k, d) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:18
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:51
reward: 43.628 ±  2.737
(k, d) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 26.876 ±  6.423
(k, d) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:13
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:06:03
reward: 40.452 ±  2.953
(k, d) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:09
reward: 23.634 ±  6.640
(k, d) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:06:00
reward: 41.328 ±  2.530
(k, d) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:09
reward: 27.366 ±  6.400
(k, d) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:13
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:05:27
reward: 43.276 ±  2.464
(k, d) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:08
reward: 23.861 ±  6.655
(k, d) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 43.687 ±  2.447
(k, d) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:07
reward: 32.498 ±  6.166
(k, d) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:43
reward: 45.848 ±  2.185
(k, d) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:03:07
reward: 28.075 ±  6.377
(k, d) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:51
reward: 43.919 ±  2.123
(k, d) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:08
reward: 27.151 ±  6.431


Thu 11 Jan 2018 10:13:09 AM PST

Laser Tag Table:

with exploration = 25
(oops: pomcpow used FORollout instead of FOValue

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 27 icaps_2018/lasertag_table.jl 
max_time = 1.0 = 1.0
max_depth = 90 = 90
N = 1000 = 1000
k = "ar_despot"
Creating Simulations...100%|████████████████████████████| Time: 0:10:01
Simulating...100%|██████████████████████████████████████| Time: 0:21:34
reward: -8.977 ±  0.188
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:42
Simulating...100%|██████████████████████████████████████| Time: 0:39:19
reward: -13.219 ±  0.226
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:23
Simulating...100%|██████████████████████████████████████| Time: 0:25:44
reward: -10.751 ±  0.193
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:23
Simulating...100%|██████████████████████████████████████| Time: 0:20:40
reward: -11.736 ±  0.155
k = "pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:21
Simulating...100%|██████████████████████████████████████| Time: 0:50:60
reward: -15.441 ±  0.195
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Thursday_11_Jan_02_35.csv...
done.


Mon 04 Dec 2017 05:11:41 PM PST

mean(combined[:mean_reward]) = -9.947048511007113
iteration 88
mean(d) = [26.3468, 3.62511, 35.0585]
ev = eigvals(cov(d)) = [2.93607e-11, 6.44598e-8, 0.00381188]
(eigvecs(cov(d)))[:, j] = [0.00247583, -0.998464, -0.0553574]
(eigvecs(cov(d)))[:, j] = [0.381521, -0.0502272, 0.922995]
(eigvecs(cov(d)))[:, j] = [-0.924357, -0.0234052, 0.38081]

Fri 01 Dec 2017 01:50:51 PM PST

Woo - discretized DESPOT works with -100, 100 bounds, and it doesn't work TOO well!

zach@Theresa:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 4 simpleld/compare.jl 
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 100 = 100
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:05:26
reward: 33.394 ±  6.001

Thu 30 Nov 2017 07:41:52 PM PST

Lasertag parameter optimization (with different problems at each iter)

iteration 8
mean(d) = [83.1987, 4.23302, 14.2264]
eigvals(cov(d)) = [0.0490264, 4.54873, 1051.37]
eigvecs(cov(d)) = [-0.0110958 0.00449547 0.999928; 0.99624 0.0859784 0.0106683; -0.0859243 0.996287 -0.00543256]
creating 80 simulation sets................................................................................
Simulating...100%|██████████████████████████████████████| Time: 0:43:31
mean(combined[:mean_reward]) = -12.143111447669922

Thu 30 Nov 2017 10:17:26 AM PST

Lasertag parameter optimization
(running on the same 80 problems each time)

Simulating...100%|██████████████████████████████████████| Time: 0:46:57
mean(combined[:mean_reward]) = -12.885963286130709
iteration 24
mean(d) = [69.2461, -2.111, 10.5063]
eigvals(cov(d)) = [0.0479565, 0.436726, 4.66563]
eigvecs(cov(d)) = [-0.101515 -0.465128 -0.879404; -0.160202 -0.864791 0.475892; 0.98185 -0.189192 -0.0132753]

Wed 22 Nov 2017 03:26:06 AM PST

simple LD results
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 500 = 500
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:04
reward: 28.599 ±  1.268
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:02:07
reward:  2.363 ±  1.737
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:05
reward:  2.499 ±  1.709
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:01:37
reward: 62.532 ±  0.715
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:14:06
reward:  2.892 ±  1.709
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:01:43
reward: 58.188 ±  0.523
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:52
reward: 30.054 ±  1.145
k = "d_despot"

Tue 21 Nov 2017 10:42:36 PM PST

Results for VDPTag 2

Tue 21 Nov 2017 05:44:55 PM PST

Acceptable results for the table for subhunt

max_time = 2.0 = 2.0
max_depth = 20 = 20
N = 1000 = 1000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
k = "ar_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:05:39
reward: 26.745 ±  1.325
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:08:12
reward: 45.500 ±  1.458
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:09:17
reward: 27.810 ±  1.342
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:06:45
reward: 74.784 ±  1.163
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:05:39
reward: 27.099 ±  1.329
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:08:45
reward: 28.159 ±  1.346
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:08:45
reward: 28.166 ±  1.346
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Tuesday_21_Nov_17_39.csv...
done.


Tue 21 Nov 2017 04:44:42 PM PST

Ok, I think we're good with the new VDPTag
max_time = 1.0 = 1.0
max_depth = 10 = 10
N = 500 = 500
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:01:08
reward: 11.808 ±  2.287
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:03:39
reward: 40.189 ±  1.119
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:01:52
reward: -16.876 ±  0.547
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:19
reward: 31.420 ±  1.316

Mon 20 Nov 2017 04:07:36 PM PST

Wooooo!

max_time = 2.0 = 2.0
max_depth = 20 = 20
N = 500 = 500
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:26
reward: 27.339 ±  1.888
k = "despot_0.1_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 26.780 ±  1.877
k = "despot_3.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 26.637 ±  1.876
k = "despot_2.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:10
reward: 27.116 ±  1.882
k = "despot_10.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:40
reward: 27.359 ±  1.890
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:17
reward: 54.165 ±  2.031
k = "despot_0.5_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:09
reward: 26.065 ±  1.863
k = "despot_5.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 24.404 ±  1.825
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 26.025 ±  1.860
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
reward: 80.111 ±  1.486
k = "despot_1.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:10
reward: 27.135 ±  1.883
k = "despot_20.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:51
reward: 27.808 ±  1.902

Sun 19 Nov 2017 08:13:42 AM PST

Sweet! with K=1000, k=4, and alpha=1/10

max_time = 10.0 = 10.0
max_depth = 20 = 20
N = 500 = 500
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:10:41
reward: 25.369 ±  1.850
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:22
reward: 27.339 ±  1.888
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
reward: 80.111 ±  1.486
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:20:13
reward: 43.446 ±  2.058

Sun 19 Nov 2017 12:49:18 AM PST

with p_aware_kill = 0.6, somewhat better

max_time = 10.0 = 10.0
max_depth = 20 = 20
N = 500 = 500
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:10:41
reward: 25.369 ±  1.850
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:22
reward: 27.339 ±  1.888
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
reward: 80.111 ±  1.486
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:20:13
reward: 43.446 ±  2.058

Sat 18 Nov 2017 11:23:36 PM PST

First subhunt... shoot

N = 500 = 500
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:01:49
reward: 25.513 ±  1.851
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:22
reward: 27.339 ±  1.888
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:17
reward: 72.263 ±  1.751
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:02:37
reward: 27.568 ±  1.895


Fri 17 Nov 2017 09:16:24 PM PST

N = 1000
solvers["despot"].lambda = 0.01
solvers["despot"].K = 500
solvers["despot"].T_max = 1.0
solvers["pomcpow"].max_time = 1.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:53
mean(rewards) = -10.807104307124584
std(rewards) / sqrt(N) = 0.1947013045198957
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:12:35
mean(rewards) = -10.920345876333318
std(rewards) / sqrt(N) = 0.16474969966372327
k = "despot"
Simulating...  1%|                                      |  ETA: 0:34:00WARNING: DESPOT's MemorizingSource
 random number generator had to move the memory locations of the rngs 457 times. If this number was large
, it may be affecting performance (profiling is the best way to tell).

To suppress this warning, use MemorizingSource(..., move_warning=false).

To reduce the number of moves, try using MemorizingSource(..., min_reserve=n) and increase n until the nu
mber of moves is low.


Fri 17 Nov 2017 07:48:28 PM PST

[ ] try pomcpow with better parameters
[ ] 

Tue 14 Nov 2017 10:35:54 PM PST

Simple light-dark

max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 20 = 20
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:06
reward: -12.400 ±  4.695
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward: -77.450 ±  8.921
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:03
reward: -74.250 ±  9.713
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:00:43
reward:  3.650 ±  0.577
k = "despot_0"
Simulating...100%|██████████████████████████████████████| Time: 0:00:07
reward: -77.450 ±  8.921
k = "heuristic_1"
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: -12.400 ±  4.695

Fri 10 Nov 2017 11:04:34 PM PST

with random numbers set

iteration 76
mean(d) = [62.9585, 18.1685, 13.0588, -0.837702, 41.5676]
eigvals(cov(d)) = [9.6914e-5, 0.00060545, 0.0842944, 0.551136, 0.711184]
eigvecs(cov(d)) = [-0.0786876 0.503232 -0.471822 -0.703022 0.153983; 0.960925 -0.139087 -0.197108 -0.0469298 0.127373; -0.19985 -0.846533 -0.169606 -0.434554 0.160746; -0.114573 -0.0917033 -0.822258 0.421748 -0.352822; -0.131775 0.0488395 -0.183461 0.369932 0.899857]
Simulating...100%|██████████████████████████████████████| Time: 0:03:40
mean(combined[:mean_reward]) = 41.016346066769394

Tue 07 Nov 2017 06:01:52 AM PST

cross-entropy maximization

iteration 100
mean(d) = [66.4341, 12.783, 7.83781, -2.42884, 33.3333]
eigvals(cov(d)) = [6.79305e-7, 0.00137267, 0.00604985, 0.241371, 2.35293]
eigvecs(cov(d)) = [0.152955 0.142402 0.639559 0.438939 -0.595503; -0.906431 -0.198838 0.202791 -0.218463 -0.223599; 0.304718 0.0804533 0.0342595 -0.822607 -0.472036; -0.245514 0.951867 -0.181687 0.00852072 -0.0242896; 0.0430942 0.166321 0.718093 -0.287839 0.609895]


Fri 20 Oct 2017 02:30:45 PM PDT

This one is with K=100

pomcpow (10, 10)
Simulating...100%|██████████████████████████████████████| Time: 0:00:58
reward: 19.824 ±  0.875
despot (10, 10)
Simulating...100%|██████████████████████████████████████| Time: 0:02:02
reward:  5.328 ±  0.936
pomcp (10, 10)
Simulating...100%|██████████████████████████████████████| Time: 0:02:05
reward:  0.833 ±  0.840
pomcpow (32, 32)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 34.254 ±  0.902
despot (32, 32)
Simulating...100%|██████████████████████████████████████| Time: 0:00:40
reward: 42.110 ±  0.705
pomcp (32, 32)
Simulating...100%|██████████████████████████████████████| Time: 0:02:17
reward: -6.254 ±  0.672
pomcpow (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 35.776 ±  0.888
despot (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:02:11
reward: 22.899 ±  0.794
pomcp (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:01:45

Fri 20 Oct 2017 02:30:13 PM PDT

[Below is with K=500 so it exceeds the 0.1 time limit]

Fri 20 Oct 2017 11:03:27 AM PDT

Oh, shoot...

pomcpow (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:00:40
reward: 38.094 ±  0.826
despot (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:03:04
reward: 51.876 ±  0.455
pomcp (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:01:44
reward:  6.676 ±  0.774

Tue 17 Oct 2017 04:54:10 PM PDT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 lasertag/discrete.jl 
N = 1000
solvers["despot"].lambda = 0.01
solvers["despot"].K = 500
solvers["despot"].T_max = 2.0
solvers["pomcpow"].max_time = 2.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:59
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:23:44
mean(rewards) = -10.554632699622246
std(rewards) / sqrt(N) = 0.16924298684972508
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:22:46
mean(rewards) = -8.670399408477682
std(rewards) / sqrt(N) = 0.18508926759230465
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Tuesday_17_Oct_15_59.jld...
done.


Thu 05 Oct 2017 10:02:18 AM PDT

Whoah - this works really well!!
Does the random policy also include random tags??

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1.0 -p 0.01 --ubtype MDP --lbtype RANDOM
Will eventually write to data/cpp_run_Thu_05_Oct_09_16.txt...
using args -n 500 -t 1.0 -p 0.01 --ubtype MDP --lbtype RANDOM
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.4s  
Average total discounted reward (stderr) = -8.493163 (0.130435)


Wed 04 Oct 2017 03:45:22 PM PDT

Ok, so to make something compelling, we have to implement the good bounds exactly, which means understanding exactly how this works.
Then we need to make a harder continuous problem
also speed tests

Wed 04 Oct 2017 02:05:38 PM PDT

TEST: does lambda decrease performance with MDP/TRIVIAL
ANSWER: lolwut. yes.

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1.0 -p 0.01 --ubtype MDP --lbtype TRIVIAL
Will eventually write to data/cpp_run_Wed_04_Oct_14_43.txt...
using args -n 500 -t 1.0 -p 0.01 --ubtype MDP --lbtype TRIVIAL
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:56/1142/100%/1.0s ^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^local:0/2000/100%/1.0s  ^[[3~^[[3~[B^[[B^[[B^[[B[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B
Average total discounted reward (stderr) = -14.074241 (0.191560)

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1.0 -p 0.0 --ubtype MDP --lbtype TRIVIAL
Will eventually write to data/cpp_run_Wed_04_Oct_14_01.txt...
using args -n 500 -t 1.0 -p 0.0 --ubtype MDP --lbtype TRIVIAL
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.1s  
Average total discounted reward (stderr) = -10.781712 (0.141706)


zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1.0 -p 0.0 --ubtype MDP --lbtype RANDOM
Will eventually write to data/cpp_run_Wed_04_Oct_14_08.txt...
using args -n 500 -t 1.0 -p 0.0 --ubtype MDP --lbtype RANDOM
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.6s  [[3~
Average total discounted reward (stderr) = -10.475910 (0.136797)

Tue 03 Oct 2017

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 10
Will eventually write to data/cpp_run_Tue_03_Oct_15_50.txt...
using args -n 500 -t 10
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/1.7s  
Average total discounted reward (stderr) = -8.720817 (0.128947)

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 5000 -t 10
Will eventually write to data/cpp_run_Tue_03_Oct_13_39.txt...
using args -n 5000 -t 10
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/2.1s  
Average total discounted reward (stderr) = -8.842181 (0.136431)

Tue 03 Oct 2017 01:39:29 PM PDT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 100 -t 0.2
Will eventually write to data/cpp_run_Tue_03_Oct_13_33.txt...
using args -n 100 -t 0.2
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.1s  
Average total discounted reward (stderr) = -9.398212 (0.126560)

Tue 03 Oct 2017 01:32:51 PM PDT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1
Will eventually write to data/cpp_run_Tue_03_Oct_13_07.txt...
using args -n 500 -t 1
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.4s  
Average total discounted reward (stderr) = -8.566343 (0.131037)


Tue 03 Oct 2017 12:03:51 PM PDT

with -n 5000 -t 10
cpp_run_Mon_02_Oct_20_20
rewards: -9.017 +/- 0.133

with -n 500 -t 1

Mon 02 Oct 2017 01:14:06 PM PDT

Trying to increase the max time with K

K = 5000
N = 1000
T_max = 10.0
k = "ardespot"
Simulating...100%|██████████████████████████████████████| Time: 0:23:36
mean(rewards) = -11.730302144935402
std(rewards) / sqrt(N) = 0.20165559476130868
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:42
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:49:44
mean(rewards) = -11.699963803538598
std(rewards) / sqrt(N) = 0.20202869275535995
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/despot_comparison_run_Monday_2_Oct_14_35.jld...

K = 500
N = 1000
T_max = 1.0
k = "ardespot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:54
mean(rewards) = -11.02352406445708
std(rewards) / sqrt(N) = 0.19018357382631232
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:43
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:02:05
mean(rewards) = -11.16220935441867
std(rewards) / sqrt(N) = 0.19411016062998934
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/despot_comparison_run_Mond
ay_2_Oct_13_11.jld...
done.


Mon 02 Oct 2017 01:01:55 PM PDT

Ok, need to run c++ despot with k=5000 and time = 10 sec?
Should probably parallelize

Mon 02 Oct 2017 11:37:59 AM PDT

lolwut

K = 5000
N = 1000
k = "ardespot"
Simulating...100%|██████████████████████████████████████| Time: 0:20:05
mean(rewards) = -11.730302144935402
std(rewards) / sqrt(N) = 0.20165559476130868
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:42
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:44:27
mean(rewards) = -11.773622775128977
std(rewards) / sqrt(N) = 0.20137185094246154
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/despot_comparison_run_Monday_2_Oct_12_53.jld...
done.


Despot comparison at K=500

N = 1000
k = "ardespot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:54
mean(rewards) = -11.02352406445708
std(rewards) / sqrt(N) = 0.19018357382631232
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:43
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:02:04
mean(rewards) = -11.09346871146992
std(rewards) / sqrt(N) = 0.19182739340444913
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/despot_comparison_run_Monday_2_Oct_11_35.jld...
done.

Fri 29 Sep 2017 09:36:08 AM PDT

At 10 it is worse

N = 1000
solvers["despot"].lambda = 0.0
solvers["despot"].K = 10
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:50
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:09
mean(rewards) = -11.419435630794528
std(rewards) / sqrt(N) = 0.17382558562220637
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_21_36.jld...

but at 50 it seems to be really good

N = 1000
solvers["despot"].lambda = 0.0
solvers["despot"].K = 50
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:50
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:08
mean(rewards) = -10.668149214716083
std(rewards) / sqrt(N) = 0.1797646933082757
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_22_04.jld...
done.

Thu 28 Sep 2017 09:31:33 PM PDT

??? Decreasing K leads to better performance

N = 1000
solvers["despot"].lambda = 0.0
solvers["despot"].K = 100
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:50
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:13
mean(rewards) = -10.85445958803187
std(rewards) / sqrt(N) = 0.18417685723205504
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_21_31.jld...
done.


Thu 28 Sep 2017 06:26:57 PM PDT

Something is very, very strange with K

solvers["despot"].lambda = 0.0
solvers["despot"].K = 50000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:21
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:25:06
mean(rewards) = -18.32498586378628
std(rewards) / sqrt(N) = 0.5187454940785285
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_19_57.jld...

solvers["despot"].lambda = 0.0
solvers["despot"].K = 5000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:22
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:39
mean(rewards) = -11.639852993326196
std(rewards) / sqrt(N) = 0.672522296221273
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_18_23.jld...
done.

solvers["despot"].lambda = 0.0
solvers["despot"].K = 500
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:21
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
mean(rewards) = -10.889650128672468
std(rewards) / sqrt(N) = 0.6570518347344256


Thu 28 Sep 2017 09:52:03 AM PDT

With old bounds

lambda = 0.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:07:58
mean(rewards) = -10.479284095896972
std(rewards) / sqrt(N) = 0.6509758212436103
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_09_50.jld...
done.

Thu 28 Sep 2017 09:28:40 AM PDT

With new bounds

WHY SO FAST??

lambda = 0.01
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:00:17
mean(rewards) = -13.028315239540785
std(rewards) / sqrt(N) = 0.6681122080230456
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Wednesday_27_Sep_23_19.jld...
done.

lambda = 0.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
mean(rewards) = -10.889650128672468
std(rewards) / sqrt(N) = 0.6570518347344256
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Wednesday_27_Sep_23_25.jld...


Mon 25 Sep 2017 02:18:39 PM PDT

N = 100
lambda = 0.01
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:08:15
mean(rewards) = -12.847838247957897
std(rewards) / sqrt(N) = 0.8827825430347918
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Wednesday_20_Sep_19_32.jld...
done.

Fri 01 Sep 2017 02:46:33 PM PDT

VDP Tag
[X] Discretization
    [X] Plot discretization fine-ness
[X] Time trend
    [X] POMCPOW time limit
    [X] POMCP time limit

Tue 29 Aug 2017 06:56:57 PM PDT

N = 100
n = 1000000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:25
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "pomcpow"
Simulating... 97%|█████████████████████████████████████ |  ETA: 0:01:26gvim: Fatal IO error 11 (Resource temporarily unavailable) on X server localhost:10.0.
Simulating...100%|██████████████████████████████████████| Time: 0:53:16
mean(rewards) = -9.5652359501832
std(rewards) / sqrt(N) = 0.5432442305531421
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:54
mean(rewards) = -23.419155908663896
std(rewards) / sqrt(N) = 2.3841840480092236
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:53:43
mean(rewards) = -10.73522490448752
std(rewards) / sqrt(N) = 0.7414608817904751
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Monday_28_Aug_23_07.jld...
done.

Thu 10 Aug 2017 02:21:02 PM PDT

N=100
n=100000
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:10:35
mean(rewards) = -12.070827601297673
std(rewards) / sqrt(N) = 0.7035574068690779


Wed 02 Aug 2017 09:19:29 PM PDT

N=100?
n=500000?
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 1:13:06
mean(rewards) = -10.606161592470805
std(rewards) / sqrt(N) = 0.6399314663360224


Wed 02 Aug 2017 05:07:51 PM PDT

n = 100_000
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:31:58
1918.487838 seconds (114.44 k allocations: 8.852 MiB)
mean(rewards) = -10.753207384302518
std(rewards) / sqrt(N) = 0.6451073093825108

Mon 05 Jun 2017 11:13:12 AM PDT

cpp despot with seed 4
Q...#......
.#.........
...........
...........
...#.......
..##...#.#.
...#.......

Sat 03 Jun 2017 05:17:16 PM PDT

[X] Look and see if any obvious parameters are off
    ? eta
[ ] Better heuristic?
[ ] Manually Verify?
[ ] Reproduce in C++
[ ] Make despot faster

Sat 03 Jun 2017 01:07:41 PM PDT

Why isn't DESPOT working well enough?

N = 100
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:32
mean(rewards) = -12.690541696637322
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 1:48:21
mean(rewards) = -13.04114149677848
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/laser_pomcpow_run_Saturday_3_Jun_02_31.jld...


Simulating...100%|██████████████████████████████████████| Time: 0:00:30
k = "qmdp"
mean(rewards) = -12.422425647531348
Simulating...100%|██████████████████████████████████████| Time: 1:15:60
k = "pomcpow"
mean(rewards) = -11.779366647775527
Simulating...100%|██████████████████████████████████████| Time: 1:49:08
k = "despot"
mean(rewards) = -12.954366015757728
Simulating...100%|██████████████████████████████████████| Time: 0:00:21
k = "move_towards_sampled"
mean(rewards) = -14.148419970637537
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/laser_pomcpow_run_Sunday_21_May_22_58.jld...


Sat 20 May 2017 10:58:43 PM PDT

With 500_000 iters
Simulating...100%|██████████████████████████████████████| Time: 0:00:32
k = "qmdp"
mean(rewards) = -12.422425647531348
Simulating...100%|██████████████████████████████████████| Time: 1:18:19
k = "pomcpow"
mean(rewards) = -11.719120527120975
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
k = "move_towards_sampled"
mean(rewards) = -14.148419970637537

Sat 20 May 2017 03:13:06 PM PDT

less aggressive dpw
enable_action_pw=true,
k_action=4.0,
alpha_action=1/8,
Simulating...100%|██████████████████████████████████████| Time: 0:05:24
k = "qmdp"
mean(rewards) = -12.789484244325477
Simulating...100%|██████████████████████████████████████| Time: 1:24:11
k = "pomcpow"
mean(rewards) = -13.675970735083467
Simulating...100%|██████████████████████████████████████| Time: 0:03:03
k = "move_towards_sampled"
mean(rewards) = -15.060258754472738
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/laser_pomcpow_run_Saturday_20_May_16_56.jld...



Sat 20 May 2017 03:11:27 PM PDT

Fixed dynamics; without action PW

Simulating...100%|██████████████████████████████████████| Time: 0:05:22
k = "qmdp"
mean(rewards) = -12.789484244325477
Simulating...100%|██████████████████████████████████████| Time: 1:38:42
k = "pomcpow"
mean(rewards) = -12.447929052529078
Simulating...100%|██████████████████████████████████████| Time: 0:03:01
k = "move_towards_sampled"
mean(rewards) = -15.060258754472738

Sat 20 May 2017 01:12:04 PM PDT

After fixing dynamics, and with action PW,

enable_action_pw=true,
k_action=4.0,
alpha_action=1/20,

Simulating...100%|██████████████████████████████████████| Time: 0:05:22
k = "qmdp"
mean(rewards) = -12.789484244325477
Simulating...100%|██████████████████████████████████████| Time: 1:28:58
k = "pomcpow"
mean(rewards) = -14.357106698498535
Simulating...100%|██████████████████████████████████████| Time: 0:03:04
k = "move_towards_sampled"
mean(rewards) = -15.060258754472738

Fri 19 May 2017 10:59:14 AM PDT

Packages that will need updating
POMCP
POMCPOW
LaserTag

Fri 19 May 2017 09:20:47 AM PDT

LaserTag:

Simulating...100%|██████████████████████████████████████| Time: 1:39:13
k = "pomcpow"
mean(rewards) = -13.319609000942341
Simulating...100%|██████████████████████████████████████| Time: 0:03:15
k = "move_towards_sampled"
mean(rewards) = -15.087908579615986

Thu 18 May 2017 09:49:53 AM PDT

[ ] Random obstacles
[ ] Visualization
[ ] Discretized
[ ] POMCP
[ ] DESPOT

Fri 12 May 2017 07:05:57 PM PDT

Forgot to copy and paste the screen, but it seems like the discretization does marginally worse
at n = 10000
discretized POMCP -> 15
pomcpow -> 20

Thu 11 May 2017 11:22:45 PM PDT

MDP discrete-doesn't seems to matter too much

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 vdptag/mcts_discretization.jl
N = 1000 = 1000
Simulating...100%|██████████████████████████████████████| Time: 0:00:06
k = "discrete_random"
mean(rewards) = -16.99111470314662
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
k = "discrete_heur"
mean(rewards) = 26.058640290930313
Simulating...100%|██████████████████████████████████████| Time: 0:10:12
k = "continuous_dpw"
mean(rewards) = 50.321368047118916
Simulating...100%|██████████████████████████████████████| Time: 0:16:49
k = "discrete_dpw"
mean(rewards) = 52.87807537064958
Simulating...100%|██████████████████████████████████████| Time: 0:20:45
k = "discrete_mcts"
mean(rewards) = 56.599055225113
Simulating...100%|██████████████████████████████████████| Time: 0:00:00
k = "continuous_heur"
mean(rewards) = 46.710457390363665

Wed 10 May 2017 11:26:24 AM PDT

MDP trends vs discrete
POMDP trends
vs Discrete
rewrite POMCP???
rewrite visualization???

Tue 09 May 2017 02:27:40 PM PDT

zsunberg@cambridge:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 10 vdptag/mcts_computation_trend.jl
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
ns[j] = 10 ^ (j / 2) = 3.1622776601683795
mrew[j] = mean(rewards) = -16.034891224228613
Simulating...100%|██████████████████████████████████████| Time: 0:00:04
ns[j] = 10 ^ (j / 2) = 10.0
mrew[j] = mean(rewards) = -10.729625355402273
Simulating...100%|██████████████████████████████████████| Time: 0:00:07
ns[j] = 10 ^ (j / 2) = 31.622776601683793
mrew[j] = mean(rewards) = 2.7637845588229917
Simulating...100%|██████████████████████████████████████| Time: 0:00:14
ns[j] = 10 ^ (j / 2) = 100.0
mrew[j] = mean(rewards) = 13.585877878818392
Simulating...100%|██████████████████████████████████████| Time: 0:00:26
ns[j] = 10 ^ (j / 2) = 316.2277660168379
mrew[j] = mean(rewards) = 30.993963847238497
Simulating...100%|██████████████████████████████████████| Time: 0:00:57
ns[j] = 10 ^ (j / 2) = 1000.0
mrew[j] = mean(rewards) = 41.132460035880214
Simulating...100%|██████████████████████████████████████| Time: 0:02:32
ns[j] = 10 ^ (j / 2) = 3162.2776601683795
mrew[j] = mean(rewards) = 45.61063910859104
Simulating...100%|██████████████████████████████████████| Time: 0:07:04
ns[j] = 10 ^ (j / 2) = 10000.0
mrew[j] = mean(rewards) = 50.353312334532276
Simulating...100%|██████████████████████████████████████| Time: 0:22:10
ns[j] = 10 ^ (j / 2) = 31622.776601683792
mrew[j] = mean(rewards) = 51.66214703274198
Simulating...100%|██████████████████████████████████████| Time: 1:04:45
ns[j] = 10 ^ (j / 2) = 100000.0
mrew[j] = mean(rewards) = 54.93820326422724
       ┌────────────────────────────────────────────────────────────┐   
    60 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ y1
       │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⡠⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠒⠒⠒⠒⠒⠒⠒⠒⠒⠒⠒⠒│   
       │⠀⠀⠀⠀⡠⠔⠒⠒⠒⠒⠒⠒⠊⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⠀⡠⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⢰⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡜⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡏⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
   -20 │⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       └────────────────────────────────────────────────────────────┘   
       0                                                       100000

Sun 07 May 2017 07:15:40 PM PDT

What tests to run????
First thoroughly check out VDP Tag and see what else we might want to investigate

DESPOT on lightdark
POMCP on lightdark?

Discretized POMCP, DESPOT, POMCP-DPW

Benchmarks from other papers


Thu 30 Mar 2017 10:34:13 PM PDT

after changing to 0.5 timestep
n=1000: 32.42
n=100:

Thu 30 Mar 2017 08:30:17 PM PDT

vdp mcts with n=1000
8.016
with n=100, 4.39

Wed 29 Mar 2017 07:03:52 PM PDT

XXX USE XXX

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl
pomcpow_10k (1 of 2)...100%|████████████████████████████| Time: 0:01:13
mean(rewards[sk]) = -9.267762047007196
sum(counts[sk]) / sum(steps[sk]) = 18464.847734227016
greedy (2 of 2)...100%|█████████████████████████████████| Time: 0:00:54
mean(rewards[sk]) = -15.753999479537239
sum(counts[sk]) / sum(steps[sk]) = 0.0
pomcpow_10k mean: -9.267762047007196 sem: 0.1707708909033004
pomcpow_10k time: 2.564552 sim counts: 253448.5 steps: 13.726
greedy mean: -15.753999479537239 sem: 0.14423709957881475
greedy time: 2.141464 sim counts: 0.0 steps: 33.686
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Wednesday_29_Mar_19_04.jld...
done.

Wed 29 Mar 2017 06:55:39 PM PDT

XXX USE XXX

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl
heuristic (1 of 6)...100%|██████████████████████████████| Time: 0:00:09
mean(rewards[sk]) = -7.033655189262772
sum(counts[sk]) / sum(steps[sk]) = 0.0
pomcpow (2 of 6)...100%|████████████████████████████████| Time: 1:19:26
mean(rewards[sk]) = -6.377730449077106
sum(counts[sk]) / sum(steps[sk]) = 2.4143038177150195e6
pomcpdpw (3 of 6)...100%|███████████████████████████████| Time: 7:33:26
mean(rewards[sk]) = -15.69943647695567
sum(counts[sk]) / sum(steps[sk]) = 319307.70218304446
bt_100_ro_100k (4 of 6)...100%|█████████████████████████| Time: 1:03:53
mean(rewards[sk]) = -5.613691760365924
sum(counts[sk]) / sum(steps[sk]) = 2.4490792149007633e7
bt_100_osv_100k (5 of 6)...100%|████████████████████████| Time: 0:47:07
mean(rewards[sk]) = -6.85277809582549
sum(counts[sk]) / sum(steps[sk]) = 8.728218322816644e6
greedy (6 of 6)...100%|█████████████████████████████████| Time: 0:00:56
mean(rewards[sk]) = -15.753999479537239
sum(counts[sk]) / sum(steps[sk]) = 0.0
heuristic mean: -7.033655189262772 sem: 0.10013644232719489
heuristic time: 0.12171999999999979 sim counts: 0.0 steps: 8.984
pomcpow mean: -6.377730449077106 sem: 0.09845233232867533
pomcpow time: 184.11048000000005 sim counts: 1.880742674e7 steps: 7.79
pomcpdpw mean: -15.69943647695567 sem: 0.1560158316692792
pomcpdpw time: 1066.607432 sim counts: 1.079451618e7 steps: 33.806
bt_100_ro_100k mean: -5.613691760365924 sem: 0.07078723454023751
bt_100_ro_100k time: 150.52855200000002 sim counts: 1.60414688576e8 steps: 6.55
bt_100_osv_100k mean: -6.85277809582549 sem: 0.12512502756233473
bt_100_osv_100k time: 109.29648800000001 sim counts: 7.6354453888e7 steps: 8.748
greedy mean: -15.753999479537239 sem: 0.14423709957881475
greedy time: 2.195144 sim counts: 0.0 steps: 33.686
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Wednesday_29_Mar_10_05.jld...
done.

Tue 28 Mar 2017 10:52:03 PM PDT

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl  bt_1000_osv_10k (1 of 9)...100%|████████████████████████| Time: 0:13:47
mean(rewards[sk]) = -11.212760358096592
sum(counts[sk]) / sum(steps[sk]) = 9.317113126448894e6
heuristic (2 of 9)...100%|██████████████████████████████| Time: 0:00:02
mean(rewards[sk]) = -7.226265473715379
sum(counts[sk]) / sum(steps[sk]) = 0.0
bt_100_ro_50k (3 of 9)...100%|██████████████████████████| Time: 0:08:40
mean(rewards[sk]) = -8.450746172323269
sum(counts[sk]) / sum(steps[sk]) = 4.5615460781512605e6
pomcpow (4 of 9)...100%|████████████████████████████████| Time: 0:17:33
mean(rewards[sk]) = -6.427691278976738
sum(counts[sk]) / sum(steps[sk]) = 2.4160163984771576e6
pomcpdpw (5 of 9)...100%|███████████████████████████████| Time: 1:40:42
mean(rewards[sk]) = -15.853267090626584
sum(counts[sk]) / sum(steps[sk]) = 319143.984297761
bt_10000_osv_1k (6 of 9)...100%|████████████████████████| Time: 0:10:28
mean(rewards[sk]) = -9.363843614136982
sum(counts[sk]) / sum(steps[sk]) = 9.489309755780347e6
bt_1000_ro_5k (7 of 9)...100%|██████████████████████████| Time: 0:06:02
mean(rewards[sk]) = -10.135463216813116
sum(counts[sk]) / sum(steps[sk]) = 4.621683939315688e6
bt_100_osv_100k (8 of 9)...100%|████████████████████████| Time: 0:11:30
mean(rewards[sk]) = -7.268103412923633
sum(counts[sk]) / sum(steps[sk]) = 8.624871826923076e6
greedy (9 of 9)...100%|█████████████████████████████████| Time: 0:00:13
mean(rewards[sk]) = -15.893171937096822
sum(counts[sk]) / sum(steps[sk]) = 0.0
bt_1000_osv_10k mean: -11.212760358096592 sem: 0.44127950612902006
bt_1000_osv_10k time: 134.07095999999999 sim counts: 1.7683880714e8 steps: 18.98
heuristic mean: -7.226265473715379 sem: 0.2725311133802234
heuristic time: 0.3929199999999998 sim counts: 0.0 steps: 9.59
bt_100_ro_50k mean: -8.450746172323269 sem: 0.35955894726607157
bt_100_ro_50k time: 70.87136 sim counts: 5.428239833e7 steps: 11.9
pomcpow mean: -6.427691278976738 sem: 0.23038042462137528
pomcpow time: 195.14068 sim counts: 1.903820922e7 steps: 7.88
pomcpdpw mean: -15.853267090626584 sem: 0.34130456588620695
pomcpdpw time: 1097.0013600000002 sim counts: 1.097536162e7 steps: 34.39
bt_10000_osv_1k mean: -9.363843614136982 sem: 0.3648557809087868
bt_10000_osv_1k time: 90.44756 sim counts: 1.3133204702e8 steps: 13.84
bt_1000_ro_5k mean: -10.135463216813116 sem: 0.3857735894005351
bt_1000_ro_5k time: 56.43364 sim counts: 7.158988422e7 steps: 15.49
bt_100_osv_100k mean: -7.268103412923633 sem: 0.28570066391523014
bt_100_osv_100k time: 114.95456 sim counts: 8.07288003e7 steps: 9.36
greedy mean: -15.893171937096822 sem: 0.30374049951337256
greedy time: 2.2675600000000005 sim counts: 0.0 steps: 34.11
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Monday_27_Mar_02_14.jld...
done.

Sun 26 Mar 2017 10:33:37 PM PDT

for bt_osv_2500, we should use

2500*100 + 2500 = 252500


Sun 26 Mar 2017 07:28:04 PM PDT

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl 
bt_osv_2500 (1 of 3)...100%|████████████████████████████| Time: 0:09:11
mean(rewards[sk]) = -9.708126278280934
sum(counts[sk]) / sum(steps[sk]) = 2375.5079695079694
heuristic (2 of 3)...100%|██████████████████████████████| Time: 0:00:02
mean(rewards[sk]) = -7.226265473715379
sum(counts[sk]) / sum(steps[sk]) = 0.0
greedy (3 of 3)...100%|█████████████████████████████████| Time: 0:00:13
mean(rewards[sk]) = -15.893171937096822
sum(counts[sk]) / sum(steps[sk]) = 0.0
bt_osv_2500 mean: -9.708126278280934 sem: 0.3601598549097225
bt_osv_2500 time: 83.33244 sim counts: 34278.58 steps: 14.43
heuristic mean: -7.226265473715379 sem: 0.2725311133802234
heuristic time: 0.4150799999999999 sim counts: 0.0 steps: 9.59
greedy mean: -15.893171937096822 sem: 0.30374049951337256
greedy time: 2.2194 sim counts: 0.0 steps: 34.11
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Sunday_26_Mar_19_25.jld...
done.


Sun 26 Mar 2017 04:54:04 PM PDT

modified_pomcp_2500 (1 of 5)...100%|████████████████████| Time: 0:04:22
mean(rewards[sk]) = -6.228689810256388
heuristic (2 of 5)...100%|██████████████████████████████| Time: 0:00:02
mean(rewards[sk]) = -7.235164761413105
pomcpow (3 of 5)...100%|████████████████████████████████| Time: 0:21:46
mean(rewards[sk]) = -7.165346048548632
modified_pomcp (4 of 5)...100%|█████████████████████████| Time: 0:15:53
mean(rewards[sk]) = -6.310329385942821
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:12
mean(rewards[sk]) = -15.905852704954135
modified_pomcp_2500 mean: -6.228689810256388 sem: 0.18214953179338803
modified_pomcp_2500 time: 43.771240000000006 sim counts: 1.4972938632e8 steps: 7.46
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
heuristic time: 0.38867999999999975 sim counts: 0.0 steps: 9.64
pomcpow mean: -7.165346048548632 sem: 0.27300311644202
pomcpow time: 225.59455999999997 sim counts: 2.205615447e7 steps: 9.23
modified_pomcp mean: -6.310329385942821 sem: 0.22239423722155335
modified_pomcp time: 162.78972000000005 sim counts: 3.6961437985e8 steps: 7.76
greedy mean: -15.905852704954135 sem: 0.305511430505056
greedy time: 2.24324 sim counts: 0.0 steps: 34.18
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Sunday_26_Mar_16_47.jld...
done.

Sun 26 Mar 2017 03:17:15 PM PDT

modified pomcp still does better than pomcpow with the same amount of simulations

Fri 24 Mar 2017 10:53:01 AM PDT

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl
heuristic (1 of 5)...100%|██████████████████████████████| Time: 0:00:08
mean(rewards[sk]) = -7.235164761413105
pomcpow (2 of 5)...100%|████████████████████████████████| Time: 0:17:41
mean(rewards[sk]) = -6.464334592747194
modified_pomcp (3 of 5)...100%|█████████████████████████| Time: 0:14:19
mean(rewards[sk]) = -6.046531249173305
vanilla_pomcp (4 of 5)...100%|██████████████████████████| Time: 1:43:55
mean(rewards[sk]) = -15.698370015626965
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:13
mean(rewards[sk]) = -16.08387713284787
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
heuristic time: 0.45967999999999987 sim counts: 0.0 steps: 9.64
pomcpow mean: -6.464334592747194 sem: 0.2422449509537489
pomcpow time: 191.7218 sim counts: 1.948591693e7 steps: 8.03
modified_pomcp mean: -6.046531249173305 sem: 0.17713801841733476
modified_pomcp time: 151.79852 sim counts: 3.4263578421e8 steps: 7.19
vanilla_pomcp mean: -15.698370015626965 sem: 0.32712445611450214
vanilla_pomcp time: 1065.6900799999999 sim counts: 1.079682272e7 steps: 33.43
greedy mean: -16.08387713284787 sem: 0.27354707984413584
greedy time: 2.3991200000000004 sim counts: 0.0 steps: 34.67
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Friday_24_Mar_00_52.jld...
done.


Wed 22 Mar 2017 09:32:00 PM PDT


heuristic (1 of 5)...100%|██████████████████████████████| Time: 0:00:07
pomcpow (2 of 5)...100%|████████████████████████████████| Time: 0:17:36
modified_pomcp (3 of 5)...100%|█████████████████████████| Time: 0:21:54
vanilla_pomcp (4 of 5)...100%|██████████████████████████| Time: 1:49:08
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:13
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
heuristic time: 0.44763999999999987 sim counts: 0.0
pomcpow mean: -6.509998772902603 sem: 0.24299155377625786
pomcpow time: 190.81344 sim counts: 2.021049328e7
modified_pomcp mean: -6.313697990295132 sem: 0.2325999439490863
modified_pomcp time: 163.98836 sim counts: 3.7512339338e8
vanilla_pomcp mean: -15.472871746113288 sem: 0.3934097251852203
vanilla_pomcp time: 1060.45316 sim counts: 1.406729023e7
greedy mean: -16.00305793721465 sem: 0.2763833162506445
greedy time: 2.3548 sim counts: 3.40177267e6
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Wednesday_22_Mar_17_25.jld...
done.


Wed 15 Mar 2017 12:27:40 PM PDT

500_000 particles in pomcpow
heuristic (1 of 5)...100%|██████████████████████████████| Time: 0:00:07
pomcpow (2 of 5)...100%|████████████████████████████████| Time: 0:16:29
modified_pomcp (3 of 5)...100%|█████████████████████████| Time: 0:23:31
vanilla_pomcp (4 of 5)...100%|██████████████████████████| Time: 0:12:50
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:21
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
pomcpow mean: -7.469768562232542 sem: 0.29152126787795163
modified_pomcp mean: -5.756805160244173 sem: 0.20696151441664185
vanilla_pomcp mean: -16.39413058834327 sem: 0.27982087036589837
greedy mean: -15.613544378353634 sem: 0.33457615002693186


1_000_000 particles in pomcpow
zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 10  --color=yes scratch/eval_pomcp.jl
heuristic (1 of 5)...100%|██████████████████████████████| Time: 0:00:08
pomcpow (2 of 5)...100%|████████████████████████████████| Time: 0:40:07
modified_pomcp (3 of 5)...100%|█████████████████████████| Time: 0:24:49
vanilla_pomcp (4 of 5)...100%|██████████████████████████| Time: 0:12:51
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:22
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
pomcpow mean: -6.679350450167447 sem: 0.27008009511089115
modified_pomcp mean: -5.909027501851269 sem: 0.2175159081441871
vanilla_pomcp mean: -16.46974571625036 sem: 0.25654700388065066
greedy mean: -15.613544378353634 sem: 0.33457615002693186

switched to SimpleParticleFilter for modified
heuristic (1 of 4)...100%|██████████████████████████████| Time: 0:00:07
modified_pomcp (2 of 4)...100%|█████████████████████████| Time: 0:19:24
vanilla_pomcp (3 of 4)...100%|██████████████████████████| Time: 0:12:58
greedy (4 of 4)...100%|█████████████████████████████████| Time: 0:00:22
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
modified_pomcp mean: -6.267678192270309 sem: 0.18293426883608033
vanilla_pomcp mean: -16.55849811620158 sem: 0.25824830249995767
greedy mean: -15.613544378353634 sem: 0.33457615002693186

Mon 06 Feb 2017 12:15:06 PM PST

[ ] Make parallel script for just one solver
[ ] Test it
[ ] Profile?
[ ] Wait for Particle Filter update

