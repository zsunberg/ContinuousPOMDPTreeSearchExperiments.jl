Tue 06 Mar 2018 12:39:10 PM PST

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018$ julia -p 54 run_all.jl

LASERTAG

max_time = 1.0 = 1.0
max_depth = 90 = 90
exploration = 26.0 = 26.0
N = 1000 = 1000
k = "qmdp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:20
Simulating...100%|██████████████████████████████████████| Time: 0:01:27
reward: -10.545 ±  0.195
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:11:34
reward: -10.324 ±  0.166
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:13:10
reward: -10.638 ±  0.189
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:12
Simulating...100%|██████████████████████████████████████| Time: 0:10:18
reward: -11.073 ±  0.161
k = "d_pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:25:13
reward: -14.124 ±  0.210
k = "despot"
Creating Simulations...100%|████████████████████████████| Time: 0:07:10
Simulating...100%|██████████████████████████████████████| Time: 0:13:12
reward: -8.902 ±  0.186
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Monday
_5_Mar_20_17.csv...
done.

SIMPLELD

max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 24.876 ±  0.861
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:09
reward: -6.369 ±  1.029
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:30:01
reward: -7.263 ±  1.000
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:03:43
reward: 56.114 ±  0.559
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:03:06
reward: 61.059 ±  0.396
k = "heuristic_1"
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 27.612 ±  0.917
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:05:21
reward: -6.819 ±  1.020
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:27:20
reward: -7.102 ±  1.004
k = "side"
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 42.417 ±  0.432
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:17
reward: 57.157 ±  0.468
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:08:17
reward: 54.167 ±  1.058
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/simpleld_Monday
_5_Mar_21_39.csv...
done.

SUBHUNT

[Iteration 1   ] residual:        100 | iteration runtime:    569.411 ms, (     0.569 s total)
[Iteration 2   ] residual:         99 | iteration runtime:    571.663 ms, (      1.14 s total)
[Iteration 3   ] residual:         98 | iteration runtime:    605.457 ms, (      1.75 s total)
[Iteration 4   ] residual:       95.5 | iteration runtime:    591.943 ms, (      2.34 s total)
[Iteration 5   ] residual:       90.8 | iteration runtime:    588.740 ms, (      2.93 s total)
[Iteration 6   ] residual:       88.9 | iteration runtime:    572.866 ms, (       3.5 s total)
[Iteration 7   ] residual:       84.4 | iteration runtime:    611.043 ms, (      4.11 s total)
[Iteration 8   ] residual:       77.8 | iteration runtime:    604.173 ms, (      4.72 s total)
[Iteration 9   ] residual:       73.7 | iteration runtime:    602.763 ms, (      5.32 s total)
[Iteration 10  ] residual:       35.2 | iteration runtime:    591.217 ms, (      5.91 s total)
[Iteration 11  ] residual:       32.7 | iteration runtime:    571.201 ms, (      6.48 s total)
[Iteration 12  ] residual:       25.8 | iteration runtime:    572.200 ms, (      7.05 s total)
[Iteration 13  ] residual:       6.78 | iteration runtime:    572.061 ms, (      7.62 s total)
[Iteration 14  ] residual:       1.69 | iteration runtime:    571.424 ms, (       8.2 s total)
[Iteration 15  ] residual:      0.272 | iteration runtime:    589.047 ms, (      8.79 s total)
[Iteration 16  ] residual:     0.0281 | iteration runtime:    572.826 ms, (      9.36 s total)
[Iteration 17  ] residual:     0.0018 | iteration runtime:    571.191 ms, (      9.93 s total)
[Iteration 18  ] residual:   7.91E-05 | iteration runtime:    575.093 ms, (      10.5 s total)
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:45
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:32
reward: 79.014 ±  1.081
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:23
reward: 69.222 ±  1.259
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:05:13
reward: 28.259 ±  1.347
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 77.361 ±  1.109
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:14
reward: 27.370 ±  1.333
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:26
reward: 27.981 ±  1.344
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 26.814 ±  1.325
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:26
reward: 28.245 ±  1.347
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Monday_
5_Mar_22_08.csv...
done.

VDPBARRIER

max_time = 1.0 = 1.0
max_depth = 10 = 10
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:00:10
reward: -2.733 ±  1.233
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:06:01
reward: 29.343 ±  0.825
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:00:13
reward: -17.055 ±  0.384
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:06:38
reward: 27.242 ±  0.839
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:12:53
reward: 16.409 ±  0.957
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:13:31
reward: 14.287 ±  0.951
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:13:08
reward: 14.669 ±  0.946
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/vdpbarrier_Mond
ay_5_Mar_23_01.csv...
done.

FILENAMES

simpleld => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/simpleld_Mond
ay_5_Mar_21_39.csv
subhunt => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Monday
_5_Mar_22_08.csv
lasertag => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Mond
ay_5_Mar_20_17.csv
vdpbarrier => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/vdpbarrier_
Monday_5_Mar_23_01.csv
filenames = Dict("simpleld"=>"/home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/
data/simpleld_Monday_5_Mar_21_39.csv","subhunt"=>"/home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExp
eriments/icaps_2018/data/subhunt_Monday_5_Mar_22_08.csv","lasertag"=>"/home/zsunberg/.julia/v0.6/Continuo
usPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Monday_5_Mar_20_17.csv","vdpbarrier"=>"/home/zsunbe
rg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/vdpbarrier_Monday_5_Mar_23_01.csv")


Thu 01 Mar 2018 11:07:39 AM PST

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/vdpbarrier_table.jl
max_time = 1.0 = 1.0
max_depth = 10 = 10
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:00:15
reward: -2.733 ±  1.233
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:06:40
reward: 29.674 ±  0.836
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:00:14
reward: -16.288 ±  0.393
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:07:11
reward: 28.178 ±  0.858
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:13:16
reward: 17.073 ±  0.950
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:14:40
reward: 13.810 ±  0.950
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:14:10
reward: 12.733 ±  0.914
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/vdpbarrier_Thursday_1_Mar_19_36.csv...

Thu 01 Mar 2018 09:04:43 AM PST

pft_opt with mean reward bmdp

Simulating...100%|██████████████████████████████████████| Time: 0:20:09
mean(combined[:mean_reward]) = 27.027060837816983
iteration 32
mean(d) = [19.3555, 69.612, 22.2074, 25.485, 7.75552, 84.81]
det(cov(d)) = 0.06679763750793188
ev = eigvals(cov(d)) = [0.0124856, 0.0190092, 0.170673, 0.743723, 9.79803, 226.293]
(eigvecs(cov(d)))[:, j] = [0.842685, -0.0518326, 0.433574, -0.204976, -0.23907, 0.00620807]
(eigvecs(cov(d)))[:, j] = [-0.320754, -0.151279, -0.00913584, -0.251411, -0.899775, -0.036677]
(eigvecs(cov(d)))[:, j] = [-0.418459, 0.0591947, 0.879368, -0.0573225, 0.152303, -0.147029]
(eigvecs(cov(d)))[:, j] = [-0.0684682, -0.421876, -0.126099, -0.829423, 0.33094, -0.0629885]
(eigvecs(cov(d)))[:, j] = [-0.0758077, 0.491786, 0.0457648, -0.312836, -0.00163842, 0.807735]
(eigvecs(cov(d)))[:, j] = [0.0381511, 0.742355, -0.143687, -0.325087, -0.0230388, -0.566211]

Tue 27 Feb 2018 11:02:59 PM PST

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018$ julia -p 51 run_all.jl

LASERTAG

max_time = 1.0 = 1.0
max_depth = 90 = 90
exploration = 26.0 = 26.0
N = 1000 = 1000
k = "qmdp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:20
Simulating...100%|██████████████████████████████████████| Time: 0:01:37
reward: -10.545 ±  0.195
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:12:51
reward: -10.275 ±  0.171
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:14:17
reward: -10.657 ±  0.191
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:12
Simulating...100%|██████████████████████████████████████| Time: 0:10:36
reward: -10.576 ±  0.162
k = "despot"
Creating Simulations...100%|████████████████████████████| Time: 0:08:09
Simulating...100%|██████████████████████████████████████| Time: 0:14:30
reward: -8.983 ±  0.180
k = "pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:29
Simulating...100%|██████████████████████████████████████| Time: 0:26:04
reward: -14.226 ±  0.211
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Monday
_26_Feb_18_46.csv...
done.

SIMPLELD

max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:03
reward: 24.876 ±  0.861
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:32:21
reward: -6.940 ±  1.006
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:03:38
reward: 57.691 ±  0.491
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 61.032 ±  0.395
k = "heuristic_1"
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 27.612 ±  0.917
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:05:29
reward: -6.819 ±  1.020
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:28:59
reward: -6.874 ±  1.009
k = "side"
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 42.417 ±  0.432
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:27
reward: 57.086 ±  0.450
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:09:18
reward: 54.609 ±  1.029
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/simpleld_Monday
_26_Feb_20_13.csv...
done.

SUBHUNT

[Iteration 1   ] residual:        100 | iteration runtime:    562.230 ms, (     0.562 s total)
[Iteration 2   ] residual:         99 | iteration runtime:    564.119 ms, (      1.13 s total)
[Iteration 3   ] residual:         98 | iteration runtime:    562.861 ms, (      1.69 s total)
[Iteration 4   ] residual:       95.5 | iteration runtime:    596.654 ms, (      2.29 s total)
[Iteration 5   ] residual:       90.8 | iteration runtime:    672.064 ms, (      2.96 s total)
[Iteration 6   ] residual:       88.9 | iteration runtime:    633.517 ms, (      3.59 s total)
[Iteration 7   ] residual:       84.4 | iteration runtime:    608.742 ms, (       4.2 s total)
[Iteration 8   ] residual:       77.8 | iteration runtime:    605.443 ms, (      4.81 s total)
[Iteration 9   ] residual:       73.7 | iteration runtime:    564.188 ms, (      5.37 s total)
[Iteration 10  ] residual:       35.2 | iteration runtime:    596.072 ms, (      5.97 s total)
[Iteration 11  ] residual:       32.7 | iteration runtime:    612.366 ms, (      6.58 s total)
[Iteration 12  ] residual:       25.8 | iteration runtime:    612.047 ms, (      7.19 s total)
[Iteration 13  ] residual:       6.78 | iteration runtime:    607.669 ms, (       7.8 s total)
[Iteration 14  ] residual:       1.69 | iteration runtime:    560.812 ms, (      8.36 s total)
[Iteration 15  ] residual:      0.272 | iteration runtime:    627.969 ms, (      8.99 s total)
[Iteration 16  ] residual:     0.0281 | iteration runtime:    614.489 ms, (       9.6 s total)
[Iteration 17  ] residual:     0.0018 | iteration runtime:    618.567 ms, (      10.2 s total)
[Iteration 18  ] residual:   7.91E-05 | iteration runtime:    615.621 ms, (      10.8 s total)
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:47
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:32
reward: 79.014 ±  1.081
k = "ar_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:22
reward: 27.002 ±  1.328
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:32
reward: 69.221 ±  1.259
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:05:27
reward: 27.614 ±  1.339
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:29
reward: 78.683 ±  1.056
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:23
reward: 27.370 ±  1.333
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 27.814 ±  1.342
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 28.786 ±  1.353
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Monday_
26_Feb_20_44.csv...
done.

VDPBARRIER

max_time = 1.0 = 1.0
max_depth = 10 = 10
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:00:17
reward: -7.969 ±  1.331
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:06:42
reward: 30.573 ±  0.829
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:00:25
reward: -17.279 ±  0.317
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:08:12
reward: 23.208 ±  0.896
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:13:46
reward: 16.035 ±  0.966
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:14:49
reward: 12.789 ±  0.926
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:13:42
reward: 16.108 ±  0.984
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/bdpbarrier_Mond
ay_26_Feb_21_42.csv...
done.

SUBHUNT_DISCRETIZATION.JL

max_depth = 20 = 20
max_time = 1.0 = 1.0
N = 1000 = 1000
max_time = 1.0
(k, binsize) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 69.147 ±  1.258
(k, binsize) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:20
reward: 28.926 ±  1.354
(k, binsize) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 27.986 ±  1.344
(k, binsize) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 69.297 ±  1.254
(k, binsize) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:21
reward: 26.500 ±  1.322
(k, binsize) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 28.334 ±  1.348
(k, binsize) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 68.457 ±  1.275
(k, binsize) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:21
reward: 26.690 ±  1.325
(k, binsize) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 28.067 ±  1.344
(k, binsize) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:39
reward: 67.440 ±  1.289
(k, binsize) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:22
reward: 25.947 ±  1.314
(k, binsize) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 27.978 ±  1.343
(k, binsize) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:40
reward: 66.194 ±  1.310
(k, binsize) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:22
reward: 27.184 ±  1.330
(k, binsize) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 28.521 ±  1.350
(k, binsize) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 68.064 ±  1.278
(k, binsize) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:03:25
reward: 26.223 ±  1.318
(k, binsize) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 28.174 ±  1.346
(k, binsize) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:33
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:05:00
reward: 44.517 ±  1.455
(k, binsize) = ("d_despot", 10.0)
Simulating... 92%|███████████████████████████████████   |  ETA: 0:00:19^[[B
Simulating...100%|██████████████████████████████████████| Time: 12:20:46
reward: 26.903 ±  1.329
(k, binsize) = ("d_pomcp", 10.0)
Simulating... 30%|████████████                          |  ETA: 0:03:21
Simulating...100%|██████████████████████████████████████| Time: 0:05:37
reward: 28.562 ±  1.355
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_discret
ization_Tuesday_27_Feb_11_38.csv...
done.

LD_DISCRETIZATION.JL

max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
(k, d) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:15
reward: 59.594 ±  0.409
(k, d) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:41
reward: 57.645 ±  0.445
(k, d) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:08:10
reward: -28.822 ±  2.424
(k, d) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:27
reward: 59.297 ±  0.414
(k, d) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:07
reward: 60.991 ±  0.412
(k, d) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:08:07
reward: -12.637 ±  2.401
(k, d) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:33
reward: 58.598 ±  0.427
(k, d) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:05:11
reward: 53.900 ±  0.771
(k, d) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:08:53
reward:  2.588 ±  2.252
(k, d) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:36
reward: 58.141 ±  0.428
(k, d) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:28:46
reward: -9.846 ±  0.824
(k, d) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:08:34
reward: 14.237 ±  2.115
(k, d) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:07
reward: 53.684 ±  0.515
(k, d) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:30:29
reward: -11.405 ±  0.838
(k, d) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:08:32
reward: 53.812 ±  1.071
(k, d) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:20
reward: 52.095 ±  0.579
(k, d) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:30:51
reward: -13.575 ±  0.707
(k, d) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:08:56
reward: 54.556 ±  1.112
(k, d) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:06:18
reward: 40.621 ±  0.911
(k, d) = ("d_pomcp", 10.0)
Simulating... 32%|████████████                          |  ETA: 0:22:05
Simulating...100%|██████████████████████████████████████| Time: 0:32:13
reward: -16.725 ±  0.536
(k, d) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:08:18
reward: 39.906 ±  1.656
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/ld_discretizati
on_Tuesday_27_Feb_15_23.csv...
done.

FILENAMES

ld_discretization.jl => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/l
d_discretization_Tuesday_27_Feb_15_23.csv
simpleld => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/simpleld_Mond
ay_26_Feb_20_13.csv
subhunt => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Monday
_26_Feb_20_44.csv
lasertag => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Mond
ay_26_Feb_18_46.csv
subhunt_discretization.jl => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/d
ata/subhunt_discretization_Tuesday_27_Feb_11_38.csv
vdpbarrier => /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/bdpbarrier_
Monday_26_Feb_21_42.csv
filenames = Dict("ld_discretization.jl"=>"/home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments
/icaps_2018/data/ld_discretization_Tuesday_27_Feb_15_23.csv","simpleld"=>"/home/zsunberg/.julia/v0.6/Cont
inuousPOMDPTreeSearchExperiments/icaps_2018/data/simpleld_Monday_26_Feb_20_13.csv","subhunt"=>"/home/zsun
berg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Monday_26_Feb_20_44.csv","l
asertag"=>"/home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Monda
y_26_Feb_18_46.csv","subhunt_discretization.jl"=>"/home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExp
eriments/icaps_2018/data/subhunt_discretization_Tuesday_27_Feb_11_38.csv","vdpbarrier"=>"/home/zsunberg/.
julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/bdpbarrier_Monday_26_Feb_21_42.csv")

Mon 26 Feb 2018 09:47:36 AM PST

pow_opt after barrier fix

iteration 16
mean(d) = [120.406, 28.0466, 30.7495, 5.46869, 114.904]
det(cov(d)) = 1.061947260056586e6
ev = eigvals(cov(d)) = [0.431383, 4.18512, 9.4225, 136.406, 457.648]
(eigvecs(cov(d)))[:, j] = [-0.0766408, -0.176956, -0.120116, 0.969829, 0.0884104]
(eigvecs(cov(d)))[:, j] = [0.345486, 0.86682, 0.242012, 0.227918, -0.136915]
(eigvecs(cov(d)))[:, j] = [-0.231256, 0.394253, -0.82353, -0.0781248, 0.326773]
(eigvecs(cov(d)))[:, j] = [0.0616476, 0.02729, 0.366556, -0.0293004, 0.927488]
(eigvecs(cov(d)))[:, j] = [0.904149, -0.247244, -0.338286, -0.0228664, 0.0801513]
creating 150 simulation sets......................................................................................................................................................
Simulating...100%|██████████████████████████████████████| Time: 0:20:18
mean(combined[:mean_reward]) = 28.22687967793777

Mon 26 Feb 2018 12:15:25 AM PST

pft_opt after barrier fix

Simulating...100%|██████████████████████████████████████| Time: 0:23:25
mean(combined[:mean_reward]) = 22.285554134050294
iteration 64
mean(d) = [14.0333, 86.7138, 18.553, 21.4697, 8.23664, 64.7801]
det(cov(d)) = 3.642743280683159e-11
ev = eigvals(cov(d)) = [7.02895e-6, 5.05806e-5, 0.00484179, 0.684519, 0.985454, 31.3709]
(eigvecs(cov(d)))[:, j] = [0.715766, 0.0888766, 0.393591, 0.112399, 0.554153, -0.0717465]
(eigvecs(cov(d)))[:, j] = [-0.605319, -0.00852499, 0.00964775, -0.0374598, 0.792353, 0.0647773]
(eigvecs(cov(d)))[:, j] = [0.324778, -0.152769, -0.897078, -0.0392837, 0.253444, 0.0256003]
(eigvecs(cov(d)))[:, j] = [0.0453787, 0.360066, -0.00923739, -0.871419, 0.0243529, -0.329004]
(eigvecs(cov(d)))[:, j] = [-0.062576, 0.892638, -0.197159, 0.394053, -0.011388, -0.0707365]
(eigvecs(cov(d)))[:, j] = [0.0990585, 0.205526, 0.0358774, -0.264142, -0.0115871, 0.936351]

Sat 24 Feb 2018 11:00:46 PM PST

After barrier fix

max_time = 1.0 = 1.0
max_depth = 10 = 10
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:02:20
reward: -6.606 ±  1.328
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:08:12
reward: 29.205 ±  0.855
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:03:55
reward: -17.460 ±  0.283
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:09:43
reward: 23.968 ±  0.918
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:14:46
reward: 15.003 ±  0.961
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:16:22
reward: 10.161 ±  0.904
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:14:35
reward: 14.884 ±  0.960

Sat 24 Feb 2018 09:13:31 PM PST

pft_opt for Random vdp tag before barrier fix

mean(combined[:mean_reward]) = 19.881941923853947
iteration 16
mean(d) = [13.7315, 75.8478, 21.2246, 21.1542, 7.64275, 63.8986]
det(cov(d)) = 138465.70897763397
ev = eigvals(cov(d)) = [0.174967, 1.61616, 3.06432, 20.7234, 62.6792, 123.023]
(eigvecs(cov(d)))[:, j] = [0.149519, 0.0428675, 0.138351, -0.135943, 0.968548, 0.0100085]
(eigvecs(cov(d)))[:, j] = [-0.883995, -0.0257854, -0.381891, 0.153053, 0.213053, 0.0568334]
(eigvecs(cov(d)))[:, j] = [-0.44218, 0.053969, 0.792449, -0.371542, -0.0978066, -0.161181]
(eigvecs(cov(d)))[:, j] = [-0.00153459, -0.361093, 0.356489, 0.81028, 0.0819346, -0.281527]
(eigvecs(cov(d)))[:, j] = [-0.0250064, -0.0159634, 0.271169, 0.201888, -0.0155503, 0.940525]
(eigvecs(cov(d)))[:, j] = [0.00677043, -0.929485, -0.0801613, -0.35034, 0.00151613, 0.0827431]

Sat 24 Feb 2018 02:07:48 PM PST

pow_opt for Random vdp tag

Simulating...100%|██████████████████████████████████████| Time: 0:21:48
mean(combined[:mean_reward]) = 25.774977146997575
iteration 48
mean(d) = [86.898, 26.0576, 28.4142, 6.85923, 105.764]
det(cov(d)) = 2.3484144732571604e-5
ev = eigvals(cov(d)) = [0.000163307, 0.00109723, 0.157391, 10.6971, 77.8439]
(eigvecs(cov(d)))[:, j] = [-0.101314, -0.00720456, 0.612271, 0.782309, -0.0529097]
(eigvecs(cov(d)))[:, j] = [0.133097, -0.34949, -0.721847, 0.581291, 0.0343396]
(eigvecs(cov(d)))[:, j] = [-0.322572, -0.879036, 0.212012, -0.202759, 0.192828]
(eigvecs(cov(d)))[:, j] = [-0.931578, 0.257634, -0.243062, 0.0689728, -0.0441557]
(eigvecs(cov(d)))[:, j] = [0.0113835, 0.196787, 0.00569271, 0.0649899, 0.978207]

Fri 23 Feb 2018 07:08:26 PM PST

max_time = 1.0 = 1.0
max_depth = 10 = 10
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:02:55
reward: -21.496 ±  1.089
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:08:55
reward: 27.340 ±  0.807
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:04:04
reward: -18.300 ±  0.240
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:10:36
reward: 21.398 ±  0.905
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:14:32
reward: 15.805 ±  0.914
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:17:32
reward:  8.548 ±  0.862
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:15:56
reward: 12.237 ±  0.877

Fri 23 Feb 2018 12:35:24 PM PST

With barriers (!!) random rollouts

max_time = 1.0 = 1.0
max_depth = 10 = 10
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:02:56
reward: -21.496 ±  1.089
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:08:50
reward: 27.484 ±  0.820
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:11:55
reward: 20.522 ±  0.905

With barriers, tonext rollouts
RO = ToNextMLSolver = VDPTag2.ToNextMLSolver
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:02:55
reward: -21.496 ±  1.089
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:16:20
reward: 17.485 ±  0.987
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:12:31
reward: 17.525 ±  0.956


Thu 22 Feb 2018 10:58:34 PM PST

mean(combined[:mean_reward]) = 35.92344192575698
iteration 16
mean(d) = [100.335, 25.5375, 19.8407, 5.45474, 107.777]
det(cov(d)) = 1703.1403575426195
ev = eigvals(cov(d)) = [0.119598, 0.404508, 2.79202, 91.3478, 138.034]
(eigvecs(cov(d)))[:, j] = [0.0705534, 0.655333, -0.258949, -0.66055, 0.249359]
(eigvecs(cov(d)))[:, j] = [0.115753, 0.589482, -0.214097, 0.748976, 0.17975]
(eigvecs(cov(d)))[:, j] = [-0.128394, -0.326238, -0.935792, 0.0170253, -0.0329793]
(eigvecs(cov(d)))[:, j] = [-0.978743, 0.184658, 0.071931, 0.0358061, -0.0388228]
(eigvecs(cov(d)))[:, j] = [-0.0848558, -0.287263, 0.0789144, 0.0337159, 0.950219]

Thu 22 Feb 2018 04:03:52 PM PST

!! With sampled rollouts
RO = RandomSolver = POMDPToolbox.RandomSolver
N = 1000 = 1000
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:06:54
reward: 35.952 ±  0.756
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:08:46
reward: 29.060 ±  0.871


Wed 21 Feb 2018 06:02:02 PM PST

PFT opt with single sample rollouts (uh oh)

Simulating...100%|██████████████████████████████████████| Time: 0:19:57
mean(combined[:mean_reward]) = 29.854269742495255
iteration 43
mean(d) = [15.2746, 86.8576, 21.0007, 20.7577, 5.59402, 54.8343]
det(cov(d)) = 2.1345807713889652e-6
ev = eigvals(cov(d)) = [0.00123411, 0.00433508, 0.0496962, 0.257932, 1.3166, 23.6417]
(eigvecs(cov(d)))[:, j] = [0.75881, 0.0841258, -0.227082, 0.218579, -0.517278, -0.224077]
(eigvecs(cov(d)))[:, j] = [0.578282, 0.295266, 0.224886, -0.0956049, 0.712911, 0.102233]
(eigvecs(cov(d)))[:, j] = [0.0420975, 0.418438, 0.28644, -0.527804, -0.442631, 0.516323]
(eigvecs(cov(d)))[:, j] = [-0.0391215, 0.200461, -0.889522, -0.114156, 0.162792, 0.357075]
(eigvecs(cov(d)))[:, j] = [0.0722885, -0.192936, -0.154998, -0.797661, 0.0411566, -0.543662]
(eigvecs(cov(d)))[:, j] = [-0.285077, 0.808233, -0.0231928, 0.123331, -0.00799412, -0.499678]

Wed 21 Feb 2018 06:01:12 PM PST

PFT opt with full belief rollouts

mean(combined[:mean_reward]) = 24.406039463499653
iteration 16
mean(d) = [10.0884, 64.2481, 16.4526, 13.9518, 4.77987, 58.2047]
det(cov(d)) = 221.66408916796289
ev = eigvals(cov(d)) = [0.0985605, 0.477854, 0.728103, 2.75178, 41.7087, 56.3202]
(eigvecs(cov(d)))[:, j] = [-0.200634, -0.0551381, -0.315826, 0.0957057, -0.920166, -0.0330813]
(eigvecs(cov(d)))[:, j] = [-0.643877, -0.114834, 0.213639, 0.702378, 0.150693, -0.102722]
(eigvecs(cov(d)))[:, j] = [0.0293745, -0.101649, 0.915678, -0.19415, -0.335313, 0.0144812]
(eigvecs(cov(d)))[:, j] = [0.70431, 0.0218577, 0.0774332, 0.675213, -0.117224, 0.166794]
(eigvecs(cov(d)))[:, j] = [0.178615, -0.873455, -0.0822278, -0.0302184, 0.0543308, -0.441075]
(eigvecs(cov(d)))[:, j] = [-0.127867, -0.458301, -0.0582189, -0.0546492, 0.0381803, 0.875087]
creating 120 simulation sets........................................................................................................................

Tue 20 Feb 2018 10:14:41 AM PST

Simulating...100%|██████████████████████████████████████| Time: 0:36:48
mean(combined[:mean_reward]) = 44.49592403115041
iteration 16
mean(d) = [11.0231, 72.3433, 25.3049, 16.8478, 27.5656, 1.58413]
det(cov(d)) = 37.838496819420826
ev = eigvals(cov(d)) = [0.00731722, 0.807921, 1.68545, 3.97169, 7.97678, 119.867]
(eigvecs(cov(d)))[:, j] = [-0.00243827, -0.00628171, -0.0748551, -0.0522666, 0.055383, 0.99426]
(eigvecs(cov(d)))[:, j] = [-0.166587, -0.0497107, 0.88766, -0.206226, -0.365498, 0.0756252]
(eigvecs(cov(d)))[:, j] = [0.283243, -0.223624, 0.178723, -0.618369, 0.672435, -0.0572258]
(eigvecs(cov(d)))[:, j] = [-0.463711, -0.548425, 0.17972, 0.511272, 0.436317, 0.0115012]
(eigvecs(cov(d)))[:, j] = [-0.822642, 0.227429, -0.225106, -0.465093, 0.0504669, -0.0447885]
(eigvecs(cov(d)))[:, j] = [0.0157834, -0.771355, -0.30256, -0.30765, -0.467179, -0.0177631]

Mon 19 Feb 2018 11:11:46 PM PST

pow opt after agent_speed = 1.5

Simulating...100%|██████████████████████████████████████| Time: 0:44:46
mean(combined[:mean_reward]) = 45.81972818279902
iteration 13
mean(d) = [21.7926, 35.0444, 14.7549, 3.33754, 97.168]
det(cov(d)) = 470.677844296718
ev = eigvals(cov(d)) = [0.0171842, 1.27947, 3.76389, 38.0314, 149.549]
(eigvecs(cov(d)))[:, j] = [0.0485464, -0.106695, 0.241261, 0.962812, -0.0323506]
(eigvecs(cov(d)))[:, j] = [0.475388, 0.304957, -0.786606, 0.211386, 0.132569]
(eigvecs(cov(d)))[:, j] = [0.78214, -0.541715, 0.18982, -0.153341, -0.187767]
(eigvecs(cov(d)))[:, j] = [0.353314, 0.773985, 0.455753, -0.0548401, -0.255731]
(eigvecs(cov(d)))[:, j] = [0.187286, 0.0557678, 0.281604, -0.0422948, 0.938469]

Mon 19 Feb 2018 10:01:58 AM PST

POMCPOW does not outperform PFT on any problems

k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:02:04
reward: 14.228 ±  1.621
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:06:26
reward: 37.260 ±  0.871
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:03:44
reward: -16.254 ±  0.412
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:05:08
reward: 36.881 ±  0.891
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:30:15
reward: -13.277 ±  0.443
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:34:39
reward: -13.120 ±  0.549
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:40:12
reward: -18.445 ±  0.257

Sat 17 Feb 2018 02:02:37 PM PST

pow opt for vdp tag after pow fix

Simulating...100%|██████████████████████████████████████| Time: 0:54:05
mean(combined[:mean_reward]) = 37.76751921884238
iteration 42
mean(d) = [31.5108, 25.9038, 8.38779, 4.96295, 96.9681]
det(cov(d)) = 1.0596841743602374e-6
ev = eigvals(cov(d)) = [2.97035e-5, 0.0614394, 0.224651, 0.609569, 4.24025]
(eigvecs(cov(d)))[:, j] = [0.0365483, -0.259587, 0.548476, 0.793156, -0.0368381]
(eigvecs(cov(d)))[:, j] = [-0.135531, 0.469818, -0.632458, 0.599298, 0.0417313]
(eigvecs(cov(d)))[:, j] = [-0.175859, 0.235181, 0.238477, -0.0368755, 0.92495]
(eigvecs(cov(d)))[:, j] = [0.72372, -0.449493, -0.375303, 0.0954354, 0.352457]
(eigvecs(cov(d)))[:, j] = [0.652379, 0.67419, 0.31851, -0.0357439, -0.130932]


Wed 14 Feb 2018 01:33:13 PM PST

RESULT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/subhunt_discr
etization.jl
max_depth = 20 = 20
max_time = 1.0 = 1.0
N = 1000 = 1000
max_time = 1.0
(k, binsize) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:47
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:32
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 62.105 ±  1.361
(k, binsize) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:19
reward: 29.104 ±  1.356
(k, binsize) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.083 ±  1.345
(k, binsize) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:32
reward: 64.007 ±  1.333
(k, binsize) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:18
reward: 26.684 ±  1.325
(k, binsize) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.168 ±  1.346
(k, binsize) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:43
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:33
reward: 62.163 ±  1.360
(k, binsize) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:18
reward: 26.690 ±  1.325
(k, binsize) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.077 ±  1.345
(k, binsize) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:31
reward: 62.531 ±  1.355
(k, binsize) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:18
reward: 25.946 ±  1.314
(k, binsize) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:34
reward: 28.269 ±  1.348
(k, binsize) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:43
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:29
reward: 63.835 ±  1.339
(k, binsize) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:18
reward: 27.278 ±  1.332
(k, binsize) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 27.355 ±  1.336
(k, binsize) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:30
reward: 61.437 ±  1.371
(k, binsize) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:03:22
reward: 26.314 ±  1.319
(k, binsize) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.632 ±  1.352
(k, binsize) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:43
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("pomcpow", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:56
reward: 39.193 ±  1.440
(k, binsize) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:42
reward: 26.627 ±  1.325
(k, binsize) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:31
reward: 28.576 ±  1.356
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_discretization_Wednesday_14_Feb_13_32.csv...
done.

Wed 14 Feb 2018 11:46:27 AM PST

RESULT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/simpleld_table.jl
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:06
reward: 24.876 ±  0.861
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:10
reward: -6.369 ±  1.029
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:32:23
reward: -7.269 ±  0.997
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:03:38
reward: 57.624 ±  0.475
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:03:03
reward: 61.182 ±  0.435
k = "heuristic_1"
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 27.612 ±  0.917
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:05:21
reward: -6.819 ±  1.020
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:29:37
reward: -7.813 ±  0.991
k = "side"
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 42.417 ±  0.432
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:04:31
reward: 49.366 ±  0.690
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:08:17
reward: 55.540 ±  0.974

Wed 14 Feb 2018 09:46:58 AM PST

RESULT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/ld_discretiza
tion.jl
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
(k, d) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:14
reward: -6.369 ±  1.029
(k, d) = ("side", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:02
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:24
reward: 59.339 ±  0.421
(k, d) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:23
reward: 57.830 ±  0.445
(k, d) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:08:25
reward: -27.790 ±  2.433
(k, d) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:10
reward: -6.369 ±  1.029
(k, d) = ("side", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:28
reward: 59.069 ±  0.428
(k, d) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:05
reward: 61.135 ±  0.410
(k, d) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:08:04
reward: -12.014 ±  2.393
(k, d) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:10
reward: -6.369 ±  1.029
(k, d) = ("side", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:29
reward: 58.200 ±  0.471
(k, d) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:05:15
reward: 53.406 ±  0.789
(k, d) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:08:05
reward:  2.324 ±  2.253
(k, d) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:36
reward: 57.817 ±  0.448
(k, d) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:29:00
reward: -10.272 ±  0.806
(k, d) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:08:20
reward: 18.855 ±  2.058
(k, d) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:10
reward: -6.369 ±  1.029
(k, d) = ("side", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:08
reward: 53.712 ±  0.536
(k, d) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:30:49
reward: -12.893 ±  0.769
(k, d) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:08:14
reward: 55.927 ±  0.912
(k, d) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward: -6.369 ±  1.029
(k, d) = ("side", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 50.634 ±  0.638
(k, d) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:30:32
reward: -12.937 ±  0.759
(k, d) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:08:14
reward: 53.911 ±  1.161
(k, d) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:10
reward: -6.369 ±  1.029
(k, d) = ("side", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: 42.417 ±  0.432
(k, d) = ("pomcpow", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:07:02
reward: 39.883 ±  0.928
(k, d) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:32:02
reward: -16.427 ±  0.555
(k, d) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:08:05
reward: 40.247 ±  1.637

Tue 13 Feb 2018 09:02:56 PM PST

RESULT subhunt time (nothing really interesting)

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/subhunt_time.
jl
max_depth = 20 = 20
N = 1000 = 1000
max_time = 0.01
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:47
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:00:36
reward: 14.869 ±  1.070
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:00:55
reward: 32.375 ±  1.387
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 0:00:34
reward: 36.367 ±  1.413
max_time = 0.03162277660168379
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:00:40
reward: 42.949 ±  1.444
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:00:59
reward: 36.877 ±  1.424
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 0:00:38
reward: 58.069 ±  1.388
max_time = 0.1
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:00:53
reward: 62.326 ±  1.360
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:01:19
reward: 44.206 ±  1.451
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 0:00:54
reward: 61.202 ±  1.363
max_time = 0.31622776601683794
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:01:31
reward: 67.596 ±  1.286
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:02:18
reward: 52.240 ±  1.436
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 0:01:37
reward: 66.222 ±  1.301
max_time = 1.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:03:38
reward: 72.706 ±  1.202
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:33
reward: 61.715 ±  1.365
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 1:02:39
reward: 71.370 ±  1.215
max_time = 3.1622776601683795
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:10:23
reward: 75.057 ±  1.156
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:11:16
reward: 74.319 ±  1.155
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 0:10:49
reward: 73.467 ±  1.178
max_time = 10.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
k = "pft_20"
Simulating...100%|██████████████████████████████████████| Time: 0:31:13
reward: 75.995 ±  1.140
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:33:39
reward: 74.901 ±  1.142
k = "pft_10"
Simulating...100%|██████████████████████████████████████| Time: 0:33:09
reward: 74.041 ±  1.173
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_time_Tuesday_13_Feb_14_03.csv...
done.

Tue 13 Feb 2018 09:00:06 PM PST

SWITCHED TO NEW LD Definition to prevent good side strategy

Mon 12 Feb 2018 07:20:31 PM PST

RESULT

k = "qmdp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:20
Simulating...100%|██████████████████████████████████████| Time: 0:01:35
reward: -10.545 ±  0.195
k = "ar_despot"
Creating Simulations...100%|████████████████████████████| Time: 0:07:46
Simulating...100%|██████████████████████████████████████| Time: 0:14:20
reward: -8.745 ±  0.180
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:19
Simulating...100%|██████████████████████████████████████| Time: 0:11:33
reward: -9.951 ±  0.176
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:14:14
reward: -10.746 ±  0.189
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:12
Simulating...100%|██████████████████████████████████████| Time: 0:11:46
reward: -11.910 ±  0.161
k = "pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:26:17
reward: -14.276 ±  0.211
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Monday_12_Feb_18_59.csv...

Mon 12 Feb 2018 10:39:56 AM PST

RESULT

zsunberg@tula:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 38 icaps_2018/vdptag2_table.jl 
max_time = 1.0 = 1.0
max_depth = 10 = 10
N = 1000 = 1000
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:03:30
reward: 14.228 ±  1.621
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:08:33
reward: 38.650 ±  0.848
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:06:35
reward: -15.811 ±  0.438
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:06:53
reward: 34.617 ±  0.870
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:41:00
reward: -11.674 ±  0.516
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:46:20
reward: -13.107 ±  0.549
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:54:34
reward: -18.505 ±  0.265


Sat 10 Feb 2018 01:05:37 PM PST

pft opt for vdptag2

mean(combined[:mean_reward]) = 34.811950775101714
iteration 39
mean(d) = [8.64351, 17.79, 12.6579, 15.6675, 6.73998]
det(cov(d)) = 1.759567322143827e-8
ev = eigvals(cov(d)) = [0.000120243, 0.0171848, 0.0604909, 0.121802, 1.15574]
(eigvecs(cov(d)))[:, j] = [-0.38064, -0.583394, 0.611271, -0.343526, -0.151995]
(eigvecs(cov(d)))[:, j] = [0.878822, -0.441938, 0.0182528, -0.175703, -0.0340483]
(eigvecs(cov(d)))[:, j] = [0.0417059, -0.228413, 0.347697, 0.70925, 0.567591]
(eigvecs(cov(d)))[:, j] = [-0.24289, -0.435707, -0.589299, -0.289784, 0.565611]
(eigvecs(cov(d)))[:, j] = [0.148496, 0.471518, 0.397302, -0.513914, 0.577636]

Tue 30 Jan 2018 12:55:25 PM PST

pow_opt for vdptag2

mean(combined[:mean_reward]) = 39.34430949569718
iteration 21
mean(d) = [63.4931, 20.5716, 10.2502, 3.47125, 81.3509]
det(cov(d)) = 1.3200977801784242
ev = eigvals(cov(d)) = [0.016337, 0.0239187, 1.29561, 14.828, 175.848]
(eigvecs(cov(d)))[:, j] = [0.0070858, 0.184705, -0.959766, 0.191787, -0.0888853]
(eigvecs(cov(d)))[:, j] = [0.0813041, 0.0143906, 0.197992, 0.976702, 0.00592561]
(eigvecs(cov(d)))[:, j] = [-0.0113952, -0.982572, -0.177943, 0.0515584, -0.0100712]
(eigvecs(cov(d)))[:, j] = [0.780962, -0.0147681, 0.0461558, -0.0703953, -0.618703]
(eigvecs(cov(d)))[:, j] = [0.619118, -0.00345987, -0.0765128, -0.0407115, 0.780493]

Sat 27 Jan 2018 04:45:16 PM PST

RESULT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/subhunt_table
.jl
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:49
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
k = "ar_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:21
reward: 27.092 ±  1.329
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:33
reward: 61.809 ±  1.367
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:05:16
reward: 27.801 ±  1.341
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:38
reward: 72.675 ±  1.205
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:03:19
reward: 27.185 ±  1.331
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 27.786 ±  1.340
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:36
reward: 27.980 ±  1.343
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Saturday_27_Jan_13_26.csv...
ERROR: LoadError: UndefVarError: CSV not defined


Fri 26 Jan 2018 04:45:55 PM PST

RESULT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/subhunt_discretization.jl
max_depth = 20 = 20
max_time = 1.0 = 1.0
N = 1000 = 1000
max_time = 1.0
(k, binsize) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:47
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:18
reward: 29.104 ±  1.356
(k, binsize) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.353 ±  1.349
(k, binsize) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:43
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 26.591 ±  1.324
(k, binsize) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.251 ±  1.347
(k, binsize) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 26.780 ±  1.327
(k, binsize) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:34
reward: 28.425 ±  1.349
(k, binsize) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:17
reward: 25.857 ±  1.313
(k, binsize) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 28.340 ±  1.348
(k, binsize) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:16
reward: 27.276 ±  1.332
(k, binsize) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:35
reward: 27.080 ±  1.332
(k, binsize) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:03:20
reward: 26.225 ±  1.318
(k, binsize) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:33
reward: 28.087 ±  1.345
(k, binsize) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 27.991 ±  1.344
(k, binsize) = ("ping_first", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:30
reward: 79.014 ±  1.081
(k, binsize) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:42
reward: 26.458 ±  1.323
(k, binsize) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:30
reward: 28.750 ±  1.358
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_time_Friday_26_Jan_14_39.csv...
done.


Thu 25 Jan 2018 01:40:53 PM PST

mean(combined[:mean_reward]) = 61.4267447713561
iteration 16
mean(d) = [17.0653, 5.7884, 47.5693]
det(cov(d)) = 1.0654999602917252
ev = eigvals(cov(d)) = [0.0127303, 0.665979, 125.677]
(eigvecs(cov(d)))[:, j] = [-0.116846, 0.991147, -0.063049]
(eigvecs(cov(d)))[:, j] = [0.985356, 0.123633, 0.117421]
(eigvecs(cov(d)))[:, j] = [-0.124177, 0.0484054, 0.991079]

Wed 24 Jan 2018 05:30:45 PM PST

Subhunt pow opt stopped

mean(combined[:mean_reward]) = 53.971569571948294
iteration 5
mean(d) = [23.4068, 6.07487, 36.9538]
det(cov(d)) = 29773.25249346697
ev = eigvals(cov(d)) = [1.92731, 65.9505, 234.237]
(eigvecs(cov(d)))[:, j] = [-0.0361602, -0.989378, 0.1408]
(eigvecs(cov(d)))[:, j] = [0.991381, -0.0532668, -0.119691]
(eigvecs(cov(d)))[:, j] = [0.125919, 0.135258, 0.982776]
creating 60 simulation sets............................................................

Wed 24 Jan 2018 10:46:06 AM PST

Subhunt pow opt

mean(combined[:mean_reward]) = 47.534730786740525
iteration 14
mean(d) = [93.601, 7.5144, 16.4266]
det(cov(d)) = 0.488007248072637
ev = eigvals(cov(d)) = [0.0248916, 0.239055, 82.0117]
(eigvecs(cov(d)))[:, j] = [-0.0478393, 0.908884, 0.414295]
(eigvecs(cov(d)))[:, j] = [-0.341551, -0.404653, 0.848292]
(eigvecs(cov(d)))[:, j] = [0.938645, -0.100921, 0.329789]

Tue 23 Jan 2018 06:04:10 PM PST

with N = 1000

zsunberg@tula:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 38 icaps_2018/simpleld_table.jl 
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:08
reward: 24.469 ±  0.854
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:05:19
reward:  6.659 ±  1.267
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:04
reward: 62.179 ±  0.511
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:34:07
reward:  5.297 ±  1.254
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 57.123 ±  0.396
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:03:42
reward: 64.496 ±  0.383
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:09:21
reward: 52.163 ±  1.346
k = "heuristic_1"
Simulating...100%|██████████████████████████████████████| Time: 0:00:03
reward: 28.216 ±  0.898
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:32:52
reward:  4.491 ±  1.238


Tue 23 Jan 2018 03:52:38 PM PST

RESULT: LD Discretization

zsunberg@tula:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 38 icaps_2018/ld_discretizatio
n.jl
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 1000 = 1000
(k, d) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:08
reward: 62.052 ±  0.496
(k, d) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:24
reward: 61.234 ±  0.561
(k, d) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:09:12
reward: -22.312 ±  2.513
(k, d) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:04:09
reward: 62.153 ±  0.438
(k, d) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:41
reward: 64.365 ±  0.419
(k, d) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:09:05
reward: -4.192 ±  2.451
(k, d) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:13
reward: 62.234 ±  0.433
(k, d) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:04:60
reward: 62.244 ±  0.573
(k, d) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:09:40
reward:  7.768 ±  2.297
(k, d) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:04:08
reward: 62.127 ±  0.504
(k, d) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:26:48
reward: 13.883 ±  1.289            
(k, d) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:09:29
reward: 23.042 ±  2.112
(k, d) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:38
reward: 58.972 ±  0.605
(k, d) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:11:47
reward: 26.626 ±  0.801
(k, d) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:09:20
reward: 51.608 ±  1.374
(k, d) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:13
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:51
reward: 57.205 ±  0.717
(k, d) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:21:46
reward: 23.825 ±  1.350
(k, d) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:09:28
reward: 51.933 ±  1.383
(k, d) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  5.287 ±  1.248
(k, d) = ("pomcpow", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:06:19
reward: 51.525 ±  0.912
(k, d) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:51
reward: 60.444 ±  0.498
(k, d) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:09:28
reward: 42.864 ±  1.711



Tue 23 Jan 2018 10:45:59 AM PST

mean(combined[:mean_reward]) = 61.980279863198554
iteration 47
mean(d) = [91.4278, 4.73208, 14.3109]
det(cov(d)) = 5.48831984150069e-8
ev = eigvals(cov(d)) = [4.31236e-7, 0.0197767, 6.43531]
(eigvecs(cov(d)))[:, j] = [-0.0269423, 0.929454, -0.367953]
(eigvecs(cov(d)))[:, j] = [0.0695339, 0.368938, 0.926849]
(eigvecs(cov(d)))[:, j] = [0.997216, -0.000613751, -0.0745686]

Mon 22 Jan 2018 02:12:00 PM PST

pow_opt for simple light-dark

mean(combined[:mean_reward]) = 61.649340998087304
iteration 10
mean(d) = [115.716, 4.18574, 13.7728]
ev = eigvals(cov(d)) = [0.202858, 1.1569, 621.191]
(eigvecs(cov(d)))[:, j] = [0.00148127, 0.999641, -0.0267648]
(eigvecs(cov(d)))[:, j] = [0.046333, 0.0266675, 0.99857]
(eigvecs(cov(d)))[:, j] = [0.998925, -0.00271925, -0.0462768]

Thu 18 Jan 2018 03:21:14 PM PST

RESULT: GOOD LASERTAG

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/lasertag_table.jl 
max_time = 1.0 = 1.0
max_depth = 90 = 90
exploration = 26.0 = 26.0
N = 1000 = 1000
k = "ar_despot"
Creating Simulations...100%|████████████████████████████| Time: 0:07:29
Simulating...100%|██████████████████████████████████████| Time: 0:13:47
reward: -8.919 ±  0.188
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:28
Simulating...100%|██████████████████████████████████████| Time: 0:12:20
reward: -10.283 ±  0.182
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:14:20
reward: -10.428 ±  0.202
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:12
Simulating... 26%|██████████                            |  ETA: 0:08:44^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[Simulating...100%|██████████████████████████████████████| Time: 0:11:05
reward: -11.586 ±  0.166
k = "pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:11
Simulating...100%|██████████████████████████████████████| Time: 0:27:05
reward: -14.497 ±  0.213
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Thursday_18_Jan_15_16.csv...
done.
zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 icaps_2018/lasertag_table.jl 
max_time = 1.0 = 1.0
max_depth = 90 = 90
exploration = 26.0 = 26.0
N = 1000 = 1000
k = "qmdp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:20
Simulating...100%|██████████████████████████████████████| Time: 0:01:37
reward: -10.486 ±  0.204
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Thursday_18_Jan_15_20.csv...
done.

Thu 11 Jan 2018 10:16:27 AM PST

Discretization preview

zach@Theresa:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018$ julia -p 6 ld_discretization.jl 
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 100 = 100
(k, d) = ("qmdp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:00:18
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:04:51
reward: 43.628 ±  2.737
(k, d) = ("d_despot", 0.01)
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 26.876 ±  6.423
(k, d) = ("qmdp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:00:13
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:06:03
reward: 40.452 ±  2.953
(k, d) = ("d_despot", 0.03162277660168379)
Simulating...100%|██████████████████████████████████████| Time: 0:03:09
reward: 23.634 ±  6.640
(k, d) = ("qmdp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:06:00
reward: 41.328 ±  2.530
(k, d) = ("d_despot", 0.1)
Simulating...100%|██████████████████████████████████████| Time: 0:03:09
reward: 27.366 ±  6.400
(k, d) = ("qmdp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:00:13
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:05:27
reward: 43.276 ±  2.464
(k, d) = ("d_despot", 0.31622776601683794)
Simulating...100%|██████████████████████████████████████| Time: 0:03:08
reward: 23.861 ±  6.655
(k, d) = ("qmdp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:11
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:37
reward: 43.687 ±  2.447
(k, d) = ("d_despot", 1.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:07
reward: 32.498 ±  6.166
(k, d) = ("qmdp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:04:43
reward: 45.848 ±  2.185
(k, d) = ("d_despot", 3.1622776601683795)
Simulating...100%|██████████████████████████████████████| Time: 0:03:07
reward: 28.075 ±  6.377
(k, d) = ("qmdp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward:  0.814 ±  3.655
(k, d) = ("d_pomcp", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:04:51
reward: 43.919 ±  2.123
(k, d) = ("d_despot", 10.0)
Simulating...100%|██████████████████████████████████████| Time: 0:03:08
reward: 27.151 ±  6.431


Thu 11 Jan 2018 10:13:09 AM PST

Laser Tag Table:

with exploration = 25
(oops: pomcpow used FORollout instead of FOValue

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 27 icaps_2018/lasertag_table.jl 
max_time = 1.0 = 1.0
max_depth = 90 = 90
N = 1000 = 1000
k = "ar_despot"
Creating Simulations...100%|████████████████████████████| Time: 0:10:01
Simulating...100%|██████████████████████████████████████| Time: 0:21:34
reward: -8.977 ±  0.188
k = "pomcpow"
Creating Simulations...100%|████████████████████████████| Time: 0:00:42
Simulating...100%|██████████████████████████████████████| Time: 0:39:19
reward: -13.219 ±  0.226
k = "pomcpdpw"
Creating Simulations...100%|████████████████████████████| Time: 0:00:23
Simulating...100%|██████████████████████████████████████| Time: 0:25:44
reward: -10.751 ±  0.193
k = "pft"
Creating Simulations...100%|████████████████████████████| Time: 0:00:23
Simulating...100%|██████████████████████████████████████| Time: 0:20:40
reward: -11.736 ±  0.155
k = "pomcp"
Creating Simulations...100%|████████████████████████████| Time: 0:00:21
Simulating...100%|██████████████████████████████████████| Time: 0:50:60
reward: -15.441 ±  0.195
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/lasertag_Thursday_11_Jan_02_35.csv...
done.


Mon 04 Dec 2017 05:11:41 PM PST

mean(combined[:mean_reward]) = -9.947048511007113
iteration 88
mean(d) = [26.3468, 3.62511, 35.0585]
ev = eigvals(cov(d)) = [2.93607e-11, 6.44598e-8, 0.00381188]
(eigvecs(cov(d)))[:, j] = [0.00247583, -0.998464, -0.0553574]
(eigvecs(cov(d)))[:, j] = [0.381521, -0.0502272, 0.922995]
(eigvecs(cov(d)))[:, j] = [-0.924357, -0.0234052, 0.38081]

Fri 01 Dec 2017 01:50:51 PM PST

Woo - discretized DESPOT works with -100, 100 bounds, and it doesn't work TOO well!

zach@Theresa:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 4 simpleld/compare.jl 
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 100 = 100
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:05:26
reward: 33.394 ±  6.001

Thu 30 Nov 2017 07:41:52 PM PST

Lasertag parameter optimization (with different problems at each iter)

iteration 8
mean(d) = [83.1987, 4.23302, 14.2264]
eigvals(cov(d)) = [0.0490264, 4.54873, 1051.37]
eigvecs(cov(d)) = [-0.0110958 0.00449547 0.999928; 0.99624 0.0859784 0.0106683; -0.0859243 0.996287 -0.00543256]
creating 80 simulation sets................................................................................
Simulating...100%|██████████████████████████████████████| Time: 0:43:31
mean(combined[:mean_reward]) = -12.143111447669922

Thu 30 Nov 2017 10:17:26 AM PST

Lasertag parameter optimization
(running on the same 80 problems each time)

Simulating...100%|██████████████████████████████████████| Time: 0:46:57
mean(combined[:mean_reward]) = -12.885963286130709
iteration 24
mean(d) = [69.2461, -2.111, 10.5063]
eigvals(cov(d)) = [0.0479565, 0.436726, 4.66563]
eigvecs(cov(d)) = [-0.101515 -0.465128 -0.879404; -0.160202 -0.864791 0.475892; 0.98185 -0.189192 -0.0132753]

Wed 22 Nov 2017 03:26:06 AM PST

simple LD results
max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 500 = 500
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:04
reward: 28.599 ±  1.268
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:02:07
reward:  2.363 ±  1.737
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:05
reward:  2.499 ±  1.709
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:01:37
reward: 62.532 ±  0.715
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:14:06
reward:  2.892 ±  1.709
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:01:43
reward: 58.188 ±  0.523
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:04:52
reward: 30.054 ±  1.145
k = "d_despot"

Tue 21 Nov 2017 10:42:36 PM PST

Results for VDPTag 2

Tue 21 Nov 2017 05:44:55 PM PST

Acceptable results for the table for subhunt

max_time = 2.0 = 2.0
max_depth = 20 = 20
N = 1000 = 1000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:46
reward: 27.991 ±  1.344
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:31
reward: 79.014 ±  1.081
k = "ar_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:05:39
reward: 26.745 ±  1.325
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:08:12
reward: 45.500 ±  1.458
k = "pomcpdpw"
Simulating...100%|██████████████████████████████████████| Time: 0:09:17
reward: 27.810 ±  1.342
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:06:45
reward: 74.784 ±  1.163
k = "d_despot"
Simulating...100%|██████████████████████████████████████| Time: 0:05:39
reward: 27.099 ±  1.329
k = "d_pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:08:45
reward: 28.159 ±  1.346
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:08:45
reward: 28.166 ±  1.346
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/icaps_2018/data/subhunt_Tuesday_21_Nov_17_39.csv...
done.


Tue 21 Nov 2017 04:44:42 PM PST

Ok, I think we're good with the new VDPTag
max_time = 1.0 = 1.0
max_depth = 10 = 10
N = 500 = 500
k = "manage_uncertainty"
Simulating...100%|██████████████████████████████████████| Time: 0:01:08
reward: 11.808 ±  2.287
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:03:39
reward: 40.189 ±  1.119
k = "to_next"
Simulating...100%|██████████████████████████████████████| Time: 0:01:52
reward: -16.876 ±  0.547
k = "pft"
Simulating...100%|██████████████████████████████████████| Time: 0:03:19
reward: 31.420 ±  1.316

Mon 20 Nov 2017 04:07:36 PM PST

Wooooo!

max_time = 2.0 = 2.0
max_depth = 20 = 20
N = 500 = 500
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:26
reward: 27.339 ±  1.888
k = "despot_0.1_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 26.780 ±  1.877
k = "despot_3.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 26.637 ±  1.876
k = "despot_2.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:10
reward: 27.116 ±  1.882
k = "despot_10.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:40
reward: 27.359 ±  1.890
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:04:17
reward: 54.165 ±  2.031
k = "despot_0.5_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:09
reward: 26.065 ±  1.863
k = "despot_5.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 24.404 ±  1.825
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:12
reward: 26.025 ±  1.860
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
reward: 80.111 ±  1.486
k = "despot_1.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:10
reward: 27.135 ±  1.883
k = "despot_20.0_01"
Simulating...100%|██████████████████████████████████████| Time: 0:03:51
reward: 27.808 ±  1.902

Sun 19 Nov 2017 08:13:42 AM PST

Sweet! with K=1000, k=4, and alpha=1/10

max_time = 10.0 = 10.0
max_depth = 20 = 20
N = 500 = 500
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:10:41
reward: 25.369 ±  1.850
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:22
reward: 27.339 ±  1.888
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
reward: 80.111 ±  1.486
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:20:13
reward: 43.446 ±  2.058

Sun 19 Nov 2017 12:49:18 AM PST

with p_aware_kill = 0.6, somewhat better

max_time = 10.0 = 10.0
max_depth = 20 = 20
N = 500 = 500
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:10:41
reward: 25.369 ±  1.850
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:22
reward: 27.339 ±  1.888
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
reward: 80.111 ±  1.486
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:20:13
reward: 43.446 ±  2.058

Sat 18 Nov 2017 11:23:36 PM PST

First subhunt... shoot

N = 500 = 500
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:01:49
reward: 25.513 ±  1.851
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:22
reward: 27.339 ±  1.888
k = "ping_first"
Simulating...100%|██████████████████████████████████████| Time: 0:00:17
reward: 72.263 ±  1.751
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:02:37
reward: 27.568 ±  1.895


Fri 17 Nov 2017 09:16:24 PM PST

N = 1000
solvers["despot"].lambda = 0.01
solvers["despot"].K = 500
solvers["despot"].T_max = 1.0
solvers["pomcpow"].max_time = 1.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:53
mean(rewards) = -10.807104307124584
std(rewards) / sqrt(N) = 0.1947013045198957
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:12:35
mean(rewards) = -10.920345876333318
std(rewards) / sqrt(N) = 0.16474969966372327
k = "despot"
Simulating...  1%|                                      |  ETA: 0:34:00WARNING: DESPOT's MemorizingSource
 random number generator had to move the memory locations of the rngs 457 times. If this number was large
, it may be affecting performance (profiling is the best way to tell).

To suppress this warning, use MemorizingSource(..., move_warning=false).

To reduce the number of moves, try using MemorizingSource(..., min_reserve=n) and increase n until the nu
mber of moves is low.


Fri 17 Nov 2017 07:48:28 PM PST

[ ] try pomcpow with better parameters
[ ] 

Tue 14 Nov 2017 10:35:54 PM PST

Simple light-dark

max_time = 1.0 = 1.0
max_depth = 20 = 20
N = 20 = 20
k = "heuristic_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:06
reward: -12.400 ±  4.695
k = "despot_01"
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
reward: -77.450 ±  8.921
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:03
reward: -74.250 ±  9.713
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:00:43
reward:  3.650 ±  0.577
k = "despot_0"
Simulating...100%|██████████████████████████████████████| Time: 0:00:07
reward: -77.450 ±  8.921
k = "heuristic_1"
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
reward: -12.400 ±  4.695

Fri 10 Nov 2017 11:04:34 PM PST

with random numbers set

iteration 76
mean(d) = [62.9585, 18.1685, 13.0588, -0.837702, 41.5676]
eigvals(cov(d)) = [9.6914e-5, 0.00060545, 0.0842944, 0.551136, 0.711184]
eigvecs(cov(d)) = [-0.0786876 0.503232 -0.471822 -0.703022 0.153983; 0.960925 -0.139087 -0.197108 -0.0469298 0.127373; -0.19985 -0.846533 -0.169606 -0.434554 0.160746; -0.114573 -0.0917033 -0.822258 0.421748 -0.352822; -0.131775 0.0488395 -0.183461 0.369932 0.899857]
Simulating...100%|██████████████████████████████████████| Time: 0:03:40
mean(combined[:mean_reward]) = 41.016346066769394

Tue 07 Nov 2017 06:01:52 AM PST

cross-entropy maximization

iteration 100
mean(d) = [66.4341, 12.783, 7.83781, -2.42884, 33.3333]
eigvals(cov(d)) = [6.79305e-7, 0.00137267, 0.00604985, 0.241371, 2.35293]
eigvecs(cov(d)) = [0.152955 0.142402 0.639559 0.438939 -0.595503; -0.906431 -0.198838 0.202791 -0.218463 -0.223599; 0.304718 0.0804533 0.0342595 -0.822607 -0.472036; -0.245514 0.951867 -0.181687 0.00852072 -0.0242896; 0.0430942 0.166321 0.718093 -0.287839 0.609895]


Fri 20 Oct 2017 02:30:45 PM PDT

This one is with K=100

pomcpow (10, 10)
Simulating...100%|██████████████████████████████████████| Time: 0:00:58
reward: 19.824 ±  0.875
despot (10, 10)
Simulating...100%|██████████████████████████████████████| Time: 0:02:02
reward:  5.328 ±  0.936
pomcp (10, 10)
Simulating...100%|██████████████████████████████████████| Time: 0:02:05
reward:  0.833 ±  0.840
pomcpow (32, 32)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 34.254 ±  0.902
despot (32, 32)
Simulating...100%|██████████████████████████████████████| Time: 0:00:40
reward: 42.110 ±  0.705
pomcp (32, 32)
Simulating...100%|██████████████████████████████████████| Time: 0:02:17
reward: -6.254 ±  0.672
pomcpow (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:00:42
reward: 35.776 ±  0.888
despot (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:02:11
reward: 22.899 ±  0.794
pomcp (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:01:45

Fri 20 Oct 2017 02:30:13 PM PDT

[Below is with K=500 so it exceeds the 0.1 time limit]

Fri 20 Oct 2017 11:03:27 AM PDT

Oh, shoot...

pomcpow (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:00:40
reward: 38.094 ±  0.826
despot (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:03:04
reward: 51.876 ±  0.455
pomcp (100, 100)
Simulating...100%|██████████████████████████████████████| Time: 0:01:44
reward:  6.676 ±  0.774

Tue 17 Oct 2017 04:54:10 PM PDT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ julia -p 51 lasertag/discrete.jl 
N = 1000
solvers["despot"].lambda = 0.01
solvers["despot"].K = 500
solvers["despot"].T_max = 2.0
solvers["pomcpow"].max_time = 2.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:59
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "pomcpow"
Simulating...100%|██████████████████████████████████████| Time: 0:23:44
mean(rewards) = -10.554632699622246
std(rewards) / sqrt(N) = 0.16924298684972508
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:22:46
mean(rewards) = -8.670399408477682
std(rewards) / sqrt(N) = 0.18508926759230465
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Tuesday_17_Oct_15_59.jld...
done.


Thu 05 Oct 2017 10:02:18 AM PDT

Whoah - this works really well!!
Does the random policy also include random tags??

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1.0 -p 0.01 --ubtype MDP --lbtype RANDOM
Will eventually write to data/cpp_run_Thu_05_Oct_09_16.txt...
using args -n 500 -t 1.0 -p 0.01 --ubtype MDP --lbtype RANDOM
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.4s  
Average total discounted reward (stderr) = -8.493163 (0.130435)


Wed 04 Oct 2017 03:45:22 PM PDT

Ok, so to make something compelling, we have to implement the good bounds exactly, which means understanding exactly how this works.
Then we need to make a harder continuous problem
also speed tests

Wed 04 Oct 2017 02:05:38 PM PDT

TEST: does lambda decrease performance with MDP/TRIVIAL
ANSWER: lolwut. yes.

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1.0 -p 0.01 --ubtype MDP --lbtype TRIVIAL
Will eventually write to data/cpp_run_Wed_04_Oct_14_43.txt...
using args -n 500 -t 1.0 -p 0.01 --ubtype MDP --lbtype TRIVIAL
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:56/1142/100%/1.0s ^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^local:0/2000/100%/1.0s  ^[[3~^[[3~[B^[[B^[[B^[[B[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B
Average total discounted reward (stderr) = -14.074241 (0.191560)

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1.0 -p 0.0 --ubtype MDP --lbtype TRIVIAL
Will eventually write to data/cpp_run_Wed_04_Oct_14_01.txt...
using args -n 500 -t 1.0 -p 0.0 --ubtype MDP --lbtype TRIVIAL
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.1s  
Average total discounted reward (stderr) = -10.781712 (0.141706)


zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1.0 -p 0.0 --ubtype MDP --lbtype RANDOM
Will eventually write to data/cpp_run_Wed_04_Oct_14_08.txt...
using args -n 500 -t 1.0 -p 0.0 --ubtype MDP --lbtype RANDOM
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.6s  [[3~
Average total discounted reward (stderr) = -10.475910 (0.136797)

Tue 03 Oct 2017

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 10
Will eventually write to data/cpp_run_Tue_03_Oct_15_50.txt...
using args -n 500 -t 10
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/1.7s  
Average total discounted reward (stderr) = -8.720817 (0.128947)

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 5000 -t 10
Will eventually write to data/cpp_run_Tue_03_Oct_13_39.txt...
using args -n 5000 -t 10
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/2.1s  
Average total discounted reward (stderr) = -8.842181 (0.136431)

Tue 03 Oct 2017 01:39:29 PM PDT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 100 -t 0.2
Will eventually write to data/cpp_run_Tue_03_Oct_13_33.txt...
using args -n 100 -t 0.2
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.1s  
Average total discounted reward (stderr) = -9.398212 (0.126560)

Tue 03 Oct 2017 01:32:51 PM PDT

zsunberg@jodhpur:~/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments$ bash cpp/run.sh 2000 -n 500 -t 1
Will eventually write to data/cpp_run_Tue_03_Oct_13_07.txt...
using args -n 500 -t 1
Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete
local:0/2000/100%/0.4s  
Average total discounted reward (stderr) = -8.566343 (0.131037)


Tue 03 Oct 2017 12:03:51 PM PDT

with -n 5000 -t 10
cpp_run_Mon_02_Oct_20_20
rewards: -9.017 +/- 0.133

with -n 500 -t 1

Mon 02 Oct 2017 01:14:06 PM PDT

Trying to increase the max time with K

K = 5000
N = 1000
T_max = 10.0
k = "ardespot"
Simulating...100%|██████████████████████████████████████| Time: 0:23:36
mean(rewards) = -11.730302144935402
std(rewards) / sqrt(N) = 0.20165559476130868
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:42
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:49:44
mean(rewards) = -11.699963803538598
std(rewards) / sqrt(N) = 0.20202869275535995
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/despot_comparison_run_Monday_2_Oct_14_35.jld...

K = 500
N = 1000
T_max = 1.0
k = "ardespot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:54
mean(rewards) = -11.02352406445708
std(rewards) / sqrt(N) = 0.19018357382631232
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:43
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:02:05
mean(rewards) = -11.16220935441867
std(rewards) / sqrt(N) = 0.19411016062998934
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/despot_comparison_run_Mond
ay_2_Oct_13_11.jld...
done.


Mon 02 Oct 2017 01:01:55 PM PDT

Ok, need to run c++ despot with k=5000 and time = 10 sec?
Should probably parallelize

Mon 02 Oct 2017 11:37:59 AM PDT

lolwut

K = 5000
N = 1000
k = "ardespot"
Simulating...100%|██████████████████████████████████████| Time: 0:20:05
mean(rewards) = -11.730302144935402
std(rewards) / sqrt(N) = 0.20165559476130868
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:42
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:44:27
mean(rewards) = -11.773622775128977
std(rewards) / sqrt(N) = 0.20137185094246154
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/despot_comparison_run_Monday_2_Oct_12_53.jld...
done.


Despot comparison at K=500

N = 1000
k = "ardespot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:54
mean(rewards) = -11.02352406445708
std(rewards) / sqrt(N) = 0.19018357382631232
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:43
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:02:04
mean(rewards) = -11.09346871146992
std(rewards) / sqrt(N) = 0.19182739340444913
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/despot_comparison_run_Monday_2_Oct_11_35.jld...
done.

Fri 29 Sep 2017 09:36:08 AM PDT

At 10 it is worse

N = 1000
solvers["despot"].lambda = 0.0
solvers["despot"].K = 10
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:50
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:09
mean(rewards) = -11.419435630794528
std(rewards) / sqrt(N) = 0.17382558562220637
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_21_36.jld...

but at 50 it seems to be really good

N = 1000
solvers["despot"].lambda = 0.0
solvers["despot"].K = 50
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:50
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:08
mean(rewards) = -10.668149214716083
std(rewards) / sqrt(N) = 0.1797646933082757
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_22_04.jld...
done.

Thu 28 Sep 2017 09:31:33 PM PDT

??? Decreasing K leads to better performance

N = 1000
solvers["despot"].lambda = 0.0
solvers["despot"].K = 100
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:01:50
mean(rewards) = -10.915843898604315
std(rewards) / sqrt(N) = 0.1959624873943656
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:13
mean(rewards) = -10.85445958803187
std(rewards) / sqrt(N) = 0.18417685723205504
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_21_31.jld...
done.


Thu 28 Sep 2017 06:26:57 PM PDT

Something is very, very strange with K

solvers["despot"].lambda = 0.0
solvers["despot"].K = 50000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:21
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:25:06
mean(rewards) = -18.32498586378628
std(rewards) / sqrt(N) = 0.5187454940785285
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_19_57.jld...

solvers["despot"].lambda = 0.0
solvers["despot"].K = 5000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:22
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:39
mean(rewards) = -11.639852993326196
std(rewards) / sqrt(N) = 0.672522296221273
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_18_23.jld...
done.

solvers["despot"].lambda = 0.0
solvers["despot"].K = 500
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:21
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
mean(rewards) = -10.889650128672468
std(rewards) / sqrt(N) = 0.6570518347344256


Thu 28 Sep 2017 09:52:03 AM PDT

With old bounds

lambda = 0.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:07:58
mean(rewards) = -10.479284095896972
std(rewards) / sqrt(N) = 0.6509758212436103
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Thursday_28_Sep_09_50.jld...
done.

Thu 28 Sep 2017 09:28:40 AM PDT

With new bounds

WHY SO FAST??

lambda = 0.01
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:00:17
mean(rewards) = -13.028315239540785
std(rewards) / sqrt(N) = 0.6681122080230456
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Wednesday_27_Sep_23_19.jld...
done.

lambda = 0.0
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:00:16
mean(rewards) = -10.889650128672468
std(rewards) / sqrt(N) = 0.6570518347344256
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Wednesday_27_Sep_23_25.jld...


Mon 25 Sep 2017 02:18:39 PM PDT

N = 100
lambda = 0.01
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:08:15
mean(rewards) = -12.847838247957897
std(rewards) / sqrt(N) = 0.8827825430347918
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Wednesday_20_Sep_19_32.jld...
done.

Fri 01 Sep 2017 02:46:33 PM PDT

VDP Tag
[X] Discretization
    [X] Plot discretization fine-ness
[X] Time trend
    [X] POMCPOW time limit
    [X] POMCP time limit

Tue 29 Aug 2017 06:56:57 PM PDT

N = 100
n = 1000000
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:25
mean(rewards) = -10.474711310004281
std(rewards) / sqrt(N) = 0.6281852344377283
k = "pomcpow"
Simulating... 97%|█████████████████████████████████████ |  ETA: 0:01:26gvim: Fatal IO error 11 (Resource temporarily unavailable) on X server localhost:10.0.
Simulating...100%|██████████████████████████████████████| Time: 0:53:16
mean(rewards) = -9.5652359501832
std(rewards) / sqrt(N) = 0.5432442305531421
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:01:54
mean(rewards) = -23.419155908663896
std(rewards) / sqrt(N) = 2.3841840480092236
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:53:43
mean(rewards) = -10.73522490448752
std(rewards) / sqrt(N) = 0.7414608817904751
saving to /home/zsunberg/.julia/v0.6/ContinuousPOMDPTreeSearchExperiments/data/laser_discrete_run_Monday_28_Aug_23_07.jld...
done.

Thu 10 Aug 2017 02:21:02 PM PDT

N=100
n=100000
k = "pomcp"
Simulating...100%|██████████████████████████████████████| Time: 0:10:35
mean(rewards) = -12.070827601297673
std(rewards) / sqrt(N) = 0.7035574068690779


Wed 02 Aug 2017 09:19:29 PM PDT

N=100?
n=500000?
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 1:13:06
mean(rewards) = -10.606161592470805
std(rewards) / sqrt(N) = 0.6399314663360224


Wed 02 Aug 2017 05:07:51 PM PDT

n = 100_000
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 0:31:58
1918.487838 seconds (114.44 k allocations: 8.852 MiB)
mean(rewards) = -10.753207384302518
std(rewards) / sqrt(N) = 0.6451073093825108

Mon 05 Jun 2017 11:13:12 AM PDT

cpp despot with seed 4
Q...#......
.#.........
...........
...........
...#.......
..##...#.#.
...#.......

Sat 03 Jun 2017 05:17:16 PM PDT

[X] Look and see if any obvious parameters are off
    ? eta
[ ] Better heuristic?
[ ] Manually Verify?
[ ] Reproduce in C++
[ ] Make despot faster

Sat 03 Jun 2017 01:07:41 PM PDT

Why isn't DESPOT working well enough?

N = 100
k = "qmdp"
Simulating...100%|██████████████████████████████████████| Time: 0:00:32
mean(rewards) = -12.690541696637322
k = "despot"
Simulating...100%|██████████████████████████████████████| Time: 1:48:21
mean(rewards) = -13.04114149677848
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/laser_pomcpow_run_Saturday_3_Jun_02_31.jld...


Simulating...100%|██████████████████████████████████████| Time: 0:00:30
k = "qmdp"
mean(rewards) = -12.422425647531348
Simulating...100%|██████████████████████████████████████| Time: 1:15:60
k = "pomcpow"
mean(rewards) = -11.779366647775527
Simulating...100%|██████████████████████████████████████| Time: 1:49:08
k = "despot"
mean(rewards) = -12.954366015757728
Simulating...100%|██████████████████████████████████████| Time: 0:00:21
k = "move_towards_sampled"
mean(rewards) = -14.148419970637537
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/laser_pomcpow_run_Sunday_21_May_22_58.jld...


Sat 20 May 2017 10:58:43 PM PDT

With 500_000 iters
Simulating...100%|██████████████████████████████████████| Time: 0:00:32
k = "qmdp"
mean(rewards) = -12.422425647531348
Simulating...100%|██████████████████████████████████████| Time: 1:18:19
k = "pomcpow"
mean(rewards) = -11.719120527120975
Simulating...100%|██████████████████████████████████████| Time: 0:00:20
k = "move_towards_sampled"
mean(rewards) = -14.148419970637537

Sat 20 May 2017 03:13:06 PM PDT

less aggressive dpw
enable_action_pw=true,
k_action=4.0,
alpha_action=1/8,
Simulating...100%|██████████████████████████████████████| Time: 0:05:24
k = "qmdp"
mean(rewards) = -12.789484244325477
Simulating...100%|██████████████████████████████████████| Time: 1:24:11
k = "pomcpow"
mean(rewards) = -13.675970735083467
Simulating...100%|██████████████████████████████████████| Time: 0:03:03
k = "move_towards_sampled"
mean(rewards) = -15.060258754472738
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/laser_pomcpow_run_Saturday_20_May_16_56.jld...



Sat 20 May 2017 03:11:27 PM PDT

Fixed dynamics; without action PW

Simulating...100%|██████████████████████████████████████| Time: 0:05:22
k = "qmdp"
mean(rewards) = -12.789484244325477
Simulating...100%|██████████████████████████████████████| Time: 1:38:42
k = "pomcpow"
mean(rewards) = -12.447929052529078
Simulating...100%|██████████████████████████████████████| Time: 0:03:01
k = "move_towards_sampled"
mean(rewards) = -15.060258754472738

Sat 20 May 2017 01:12:04 PM PDT

After fixing dynamics, and with action PW,

enable_action_pw=true,
k_action=4.0,
alpha_action=1/20,

Simulating...100%|██████████████████████████████████████| Time: 0:05:22
k = "qmdp"
mean(rewards) = -12.789484244325477
Simulating...100%|██████████████████████████████████████| Time: 1:28:58
k = "pomcpow"
mean(rewards) = -14.357106698498535
Simulating...100%|██████████████████████████████████████| Time: 0:03:04
k = "move_towards_sampled"
mean(rewards) = -15.060258754472738

Fri 19 May 2017 10:59:14 AM PDT

Packages that will need updating
POMCP
POMCPOW
LaserTag

Fri 19 May 2017 09:20:47 AM PDT

LaserTag:

Simulating...100%|██████████████████████████████████████| Time: 1:39:13
k = "pomcpow"
mean(rewards) = -13.319609000942341
Simulating...100%|██████████████████████████████████████| Time: 0:03:15
k = "move_towards_sampled"
mean(rewards) = -15.087908579615986

Thu 18 May 2017 09:49:53 AM PDT

[ ] Random obstacles
[ ] Visualization
[ ] Discretized
[ ] POMCP
[ ] DESPOT

Fri 12 May 2017 07:05:57 PM PDT

Forgot to copy and paste the screen, but it seems like the discretization does marginally worse
at n = 10000
discretized POMCP -> 15
pomcpow -> 20

Thu 11 May 2017 11:22:45 PM PDT

MDP discrete-doesn't seems to matter too much

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 vdptag/mcts_discretization.jl
N = 1000 = 1000
Simulating...100%|██████████████████████████████████████| Time: 0:00:06
k = "discrete_random"
mean(rewards) = -16.99111470314662
Simulating...100%|██████████████████████████████████████| Time: 0:00:01
k = "discrete_heur"
mean(rewards) = 26.058640290930313
Simulating...100%|██████████████████████████████████████| Time: 0:10:12
k = "continuous_dpw"
mean(rewards) = 50.321368047118916
Simulating...100%|██████████████████████████████████████| Time: 0:16:49
k = "discrete_dpw"
mean(rewards) = 52.87807537064958
Simulating...100%|██████████████████████████████████████| Time: 0:20:45
k = "discrete_mcts"
mean(rewards) = 56.599055225113
Simulating...100%|██████████████████████████████████████| Time: 0:00:00
k = "continuous_heur"
mean(rewards) = 46.710457390363665

Wed 10 May 2017 11:26:24 AM PDT

MDP trends vs discrete
POMDP trends
vs Discrete
rewrite POMCP???
rewrite visualization???

Tue 09 May 2017 02:27:40 PM PDT

zsunberg@cambridge:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 10 vdptag/mcts_computation_trend.jl
Simulating...100%|██████████████████████████████████████| Time: 0:00:12
ns[j] = 10 ^ (j / 2) = 3.1622776601683795
mrew[j] = mean(rewards) = -16.034891224228613
Simulating...100%|██████████████████████████████████████| Time: 0:00:04
ns[j] = 10 ^ (j / 2) = 10.0
mrew[j] = mean(rewards) = -10.729625355402273
Simulating...100%|██████████████████████████████████████| Time: 0:00:07
ns[j] = 10 ^ (j / 2) = 31.622776601683793
mrew[j] = mean(rewards) = 2.7637845588229917
Simulating...100%|██████████████████████████████████████| Time: 0:00:14
ns[j] = 10 ^ (j / 2) = 100.0
mrew[j] = mean(rewards) = 13.585877878818392
Simulating...100%|██████████████████████████████████████| Time: 0:00:26
ns[j] = 10 ^ (j / 2) = 316.2277660168379
mrew[j] = mean(rewards) = 30.993963847238497
Simulating...100%|██████████████████████████████████████| Time: 0:00:57
ns[j] = 10 ^ (j / 2) = 1000.0
mrew[j] = mean(rewards) = 41.132460035880214
Simulating...100%|██████████████████████████████████████| Time: 0:02:32
ns[j] = 10 ^ (j / 2) = 3162.2776601683795
mrew[j] = mean(rewards) = 45.61063910859104
Simulating...100%|██████████████████████████████████████| Time: 0:07:04
ns[j] = 10 ^ (j / 2) = 10000.0
mrew[j] = mean(rewards) = 50.353312334532276
Simulating...100%|██████████████████████████████████████| Time: 0:22:10
ns[j] = 10 ^ (j / 2) = 31622.776601683792
mrew[j] = mean(rewards) = 51.66214703274198
Simulating...100%|██████████████████████████████████████| Time: 1:04:45
ns[j] = 10 ^ (j / 2) = 100000.0
mrew[j] = mean(rewards) = 54.93820326422724
       ┌────────────────────────────────────────────────────────────┐   
    60 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ y1
       │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⡠⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠒⠒⠒⠒⠒⠒⠒⠒⠒⠒⠒⠒│   
       │⠀⠀⠀⠀⡠⠔⠒⠒⠒⠒⠒⠒⠊⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⠀⡠⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⢰⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡜⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡏⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠁│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       │⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
   -20 │⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│   
       └────────────────────────────────────────────────────────────┘   
       0                                                       100000

Sun 07 May 2017 07:15:40 PM PDT

What tests to run????
First thoroughly check out VDP Tag and see what else we might want to investigate

DESPOT on lightdark
POMCP on lightdark?

Discretized POMCP, DESPOT, POMCP-DPW

Benchmarks from other papers


Thu 30 Mar 2017 10:34:13 PM PDT

after changing to 0.5 timestep
n=1000: 32.42
n=100:

Thu 30 Mar 2017 08:30:17 PM PDT

vdp mcts with n=1000
8.016
with n=100, 4.39

Wed 29 Mar 2017 07:03:52 PM PDT

XXX USE XXX

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl
pomcpow_10k (1 of 2)...100%|████████████████████████████| Time: 0:01:13
mean(rewards[sk]) = -9.267762047007196
sum(counts[sk]) / sum(steps[sk]) = 18464.847734227016
greedy (2 of 2)...100%|█████████████████████████████████| Time: 0:00:54
mean(rewards[sk]) = -15.753999479537239
sum(counts[sk]) / sum(steps[sk]) = 0.0
pomcpow_10k mean: -9.267762047007196 sem: 0.1707708909033004
pomcpow_10k time: 2.564552 sim counts: 253448.5 steps: 13.726
greedy mean: -15.753999479537239 sem: 0.14423709957881475
greedy time: 2.141464 sim counts: 0.0 steps: 33.686
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Wednesday_29_Mar_19_04.jld...
done.

Wed 29 Mar 2017 06:55:39 PM PDT

XXX USE XXX

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl
heuristic (1 of 6)...100%|██████████████████████████████| Time: 0:00:09
mean(rewards[sk]) = -7.033655189262772
sum(counts[sk]) / sum(steps[sk]) = 0.0
pomcpow (2 of 6)...100%|████████████████████████████████| Time: 1:19:26
mean(rewards[sk]) = -6.377730449077106
sum(counts[sk]) / sum(steps[sk]) = 2.4143038177150195e6
pomcpdpw (3 of 6)...100%|███████████████████████████████| Time: 7:33:26
mean(rewards[sk]) = -15.69943647695567
sum(counts[sk]) / sum(steps[sk]) = 319307.70218304446
bt_100_ro_100k (4 of 6)...100%|█████████████████████████| Time: 1:03:53
mean(rewards[sk]) = -5.613691760365924
sum(counts[sk]) / sum(steps[sk]) = 2.4490792149007633e7
bt_100_osv_100k (5 of 6)...100%|████████████████████████| Time: 0:47:07
mean(rewards[sk]) = -6.85277809582549
sum(counts[sk]) / sum(steps[sk]) = 8.728218322816644e6
greedy (6 of 6)...100%|█████████████████████████████████| Time: 0:00:56
mean(rewards[sk]) = -15.753999479537239
sum(counts[sk]) / sum(steps[sk]) = 0.0
heuristic mean: -7.033655189262772 sem: 0.10013644232719489
heuristic time: 0.12171999999999979 sim counts: 0.0 steps: 8.984
pomcpow mean: -6.377730449077106 sem: 0.09845233232867533
pomcpow time: 184.11048000000005 sim counts: 1.880742674e7 steps: 7.79
pomcpdpw mean: -15.69943647695567 sem: 0.1560158316692792
pomcpdpw time: 1066.607432 sim counts: 1.079451618e7 steps: 33.806
bt_100_ro_100k mean: -5.613691760365924 sem: 0.07078723454023751
bt_100_ro_100k time: 150.52855200000002 sim counts: 1.60414688576e8 steps: 6.55
bt_100_osv_100k mean: -6.85277809582549 sem: 0.12512502756233473
bt_100_osv_100k time: 109.29648800000001 sim counts: 7.6354453888e7 steps: 8.748
greedy mean: -15.753999479537239 sem: 0.14423709957881475
greedy time: 2.195144 sim counts: 0.0 steps: 33.686
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Wednesday_29_Mar_10_05.jld...
done.

Tue 28 Mar 2017 10:52:03 PM PDT

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl  bt_1000_osv_10k (1 of 9)...100%|████████████████████████| Time: 0:13:47
mean(rewards[sk]) = -11.212760358096592
sum(counts[sk]) / sum(steps[sk]) = 9.317113126448894e6
heuristic (2 of 9)...100%|██████████████████████████████| Time: 0:00:02
mean(rewards[sk]) = -7.226265473715379
sum(counts[sk]) / sum(steps[sk]) = 0.0
bt_100_ro_50k (3 of 9)...100%|██████████████████████████| Time: 0:08:40
mean(rewards[sk]) = -8.450746172323269
sum(counts[sk]) / sum(steps[sk]) = 4.5615460781512605e6
pomcpow (4 of 9)...100%|████████████████████████████████| Time: 0:17:33
mean(rewards[sk]) = -6.427691278976738
sum(counts[sk]) / sum(steps[sk]) = 2.4160163984771576e6
pomcpdpw (5 of 9)...100%|███████████████████████████████| Time: 1:40:42
mean(rewards[sk]) = -15.853267090626584
sum(counts[sk]) / sum(steps[sk]) = 319143.984297761
bt_10000_osv_1k (6 of 9)...100%|████████████████████████| Time: 0:10:28
mean(rewards[sk]) = -9.363843614136982
sum(counts[sk]) / sum(steps[sk]) = 9.489309755780347e6
bt_1000_ro_5k (7 of 9)...100%|██████████████████████████| Time: 0:06:02
mean(rewards[sk]) = -10.135463216813116
sum(counts[sk]) / sum(steps[sk]) = 4.621683939315688e6
bt_100_osv_100k (8 of 9)...100%|████████████████████████| Time: 0:11:30
mean(rewards[sk]) = -7.268103412923633
sum(counts[sk]) / sum(steps[sk]) = 8.624871826923076e6
greedy (9 of 9)...100%|█████████████████████████████████| Time: 0:00:13
mean(rewards[sk]) = -15.893171937096822
sum(counts[sk]) / sum(steps[sk]) = 0.0
bt_1000_osv_10k mean: -11.212760358096592 sem: 0.44127950612902006
bt_1000_osv_10k time: 134.07095999999999 sim counts: 1.7683880714e8 steps: 18.98
heuristic mean: -7.226265473715379 sem: 0.2725311133802234
heuristic time: 0.3929199999999998 sim counts: 0.0 steps: 9.59
bt_100_ro_50k mean: -8.450746172323269 sem: 0.35955894726607157
bt_100_ro_50k time: 70.87136 sim counts: 5.428239833e7 steps: 11.9
pomcpow mean: -6.427691278976738 sem: 0.23038042462137528
pomcpow time: 195.14068 sim counts: 1.903820922e7 steps: 7.88
pomcpdpw mean: -15.853267090626584 sem: 0.34130456588620695
pomcpdpw time: 1097.0013600000002 sim counts: 1.097536162e7 steps: 34.39
bt_10000_osv_1k mean: -9.363843614136982 sem: 0.3648557809087868
bt_10000_osv_1k time: 90.44756 sim counts: 1.3133204702e8 steps: 13.84
bt_1000_ro_5k mean: -10.135463216813116 sem: 0.3857735894005351
bt_1000_ro_5k time: 56.43364 sim counts: 7.158988422e7 steps: 15.49
bt_100_osv_100k mean: -7.268103412923633 sem: 0.28570066391523014
bt_100_osv_100k time: 114.95456 sim counts: 8.07288003e7 steps: 9.36
greedy mean: -15.893171937096822 sem: 0.30374049951337256
greedy time: 2.2675600000000005 sim counts: 0.0 steps: 34.11
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Monday_27_Mar_02_14.jld...
done.

Sun 26 Mar 2017 10:33:37 PM PDT

for bt_osv_2500, we should use

2500*100 + 2500 = 252500


Sun 26 Mar 2017 07:28:04 PM PDT

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl 
bt_osv_2500 (1 of 3)...100%|████████████████████████████| Time: 0:09:11
mean(rewards[sk]) = -9.708126278280934
sum(counts[sk]) / sum(steps[sk]) = 2375.5079695079694
heuristic (2 of 3)...100%|██████████████████████████████| Time: 0:00:02
mean(rewards[sk]) = -7.226265473715379
sum(counts[sk]) / sum(steps[sk]) = 0.0
greedy (3 of 3)...100%|█████████████████████████████████| Time: 0:00:13
mean(rewards[sk]) = -15.893171937096822
sum(counts[sk]) / sum(steps[sk]) = 0.0
bt_osv_2500 mean: -9.708126278280934 sem: 0.3601598549097225
bt_osv_2500 time: 83.33244 sim counts: 34278.58 steps: 14.43
heuristic mean: -7.226265473715379 sem: 0.2725311133802234
heuristic time: 0.4150799999999999 sim counts: 0.0 steps: 9.59
greedy mean: -15.893171937096822 sem: 0.30374049951337256
greedy time: 2.2194 sim counts: 0.0 steps: 34.11
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Sunday_26_Mar_19_25.jld...
done.


Sun 26 Mar 2017 04:54:04 PM PDT

modified_pomcp_2500 (1 of 5)...100%|████████████████████| Time: 0:04:22
mean(rewards[sk]) = -6.228689810256388
heuristic (2 of 5)...100%|██████████████████████████████| Time: 0:00:02
mean(rewards[sk]) = -7.235164761413105
pomcpow (3 of 5)...100%|████████████████████████████████| Time: 0:21:46
mean(rewards[sk]) = -7.165346048548632
modified_pomcp (4 of 5)...100%|█████████████████████████| Time: 0:15:53
mean(rewards[sk]) = -6.310329385942821
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:12
mean(rewards[sk]) = -15.905852704954135
modified_pomcp_2500 mean: -6.228689810256388 sem: 0.18214953179338803
modified_pomcp_2500 time: 43.771240000000006 sim counts: 1.4972938632e8 steps: 7.46
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
heuristic time: 0.38867999999999975 sim counts: 0.0 steps: 9.64
pomcpow mean: -7.165346048548632 sem: 0.27300311644202
pomcpow time: 225.59455999999997 sim counts: 2.205615447e7 steps: 9.23
modified_pomcp mean: -6.310329385942821 sem: 0.22239423722155335
modified_pomcp time: 162.78972000000005 sim counts: 3.6961437985e8 steps: 7.76
greedy mean: -15.905852704954135 sem: 0.305511430505056
greedy time: 2.24324 sim counts: 0.0 steps: 34.18
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Sunday_26_Mar_16_47.jld...
done.

Sun 26 Mar 2017 03:17:15 PM PDT

modified pomcp still does better than pomcpow with the same amount of simulations

Fri 24 Mar 2017 10:53:01 AM PDT

zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 20 uai_2017/ld_comparison.jl
heuristic (1 of 5)...100%|██████████████████████████████| Time: 0:00:08
mean(rewards[sk]) = -7.235164761413105
pomcpow (2 of 5)...100%|████████████████████████████████| Time: 0:17:41
mean(rewards[sk]) = -6.464334592747194
modified_pomcp (3 of 5)...100%|█████████████████████████| Time: 0:14:19
mean(rewards[sk]) = -6.046531249173305
vanilla_pomcp (4 of 5)...100%|██████████████████████████| Time: 1:43:55
mean(rewards[sk]) = -15.698370015626965
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:13
mean(rewards[sk]) = -16.08387713284787
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
heuristic time: 0.45967999999999987 sim counts: 0.0 steps: 9.64
pomcpow mean: -6.464334592747194 sem: 0.2422449509537489
pomcpow time: 191.7218 sim counts: 1.948591693e7 steps: 8.03
modified_pomcp mean: -6.046531249173305 sem: 0.17713801841733476
modified_pomcp time: 151.79852 sim counts: 3.4263578421e8 steps: 7.19
vanilla_pomcp mean: -15.698370015626965 sem: 0.32712445611450214
vanilla_pomcp time: 1065.6900799999999 sim counts: 1.079682272e7 steps: 33.43
greedy mean: -16.08387713284787 sem: 0.27354707984413584
greedy time: 2.3991200000000004 sim counts: 0.0 steps: 34.67
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Friday_24_Mar_00_52.jld...
done.


Wed 22 Mar 2017 09:32:00 PM PDT


heuristic (1 of 5)...100%|██████████████████████████████| Time: 0:00:07
pomcpow (2 of 5)...100%|████████████████████████████████| Time: 0:17:36
modified_pomcp (3 of 5)...100%|█████████████████████████| Time: 0:21:54
vanilla_pomcp (4 of 5)...100%|██████████████████████████| Time: 1:49:08
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:13
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
heuristic time: 0.44763999999999987 sim counts: 0.0
pomcpow mean: -6.509998772902603 sem: 0.24299155377625786
pomcpow time: 190.81344 sim counts: 2.021049328e7
modified_pomcp mean: -6.313697990295132 sem: 0.2325999439490863
modified_pomcp time: 163.98836 sim counts: 3.7512339338e8
vanilla_pomcp mean: -15.472871746113288 sem: 0.3934097251852203
vanilla_pomcp time: 1060.45316 sim counts: 1.406729023e7
greedy mean: -16.00305793721465 sem: 0.2763833162506445
greedy time: 2.3548 sim counts: 3.40177267e6
saving to /home/zsunberg/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments/data/compare_Wednesday_22_Mar_17_25.jld...
done.


Wed 15 Mar 2017 12:27:40 PM PDT

500_000 particles in pomcpow
heuristic (1 of 5)...100%|██████████████████████████████| Time: 0:00:07
pomcpow (2 of 5)...100%|████████████████████████████████| Time: 0:16:29
modified_pomcp (3 of 5)...100%|█████████████████████████| Time: 0:23:31
vanilla_pomcp (4 of 5)...100%|██████████████████████████| Time: 0:12:50
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:21
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
pomcpow mean: -7.469768562232542 sem: 0.29152126787795163
modified_pomcp mean: -5.756805160244173 sem: 0.20696151441664185
vanilla_pomcp mean: -16.39413058834327 sem: 0.27982087036589837
greedy mean: -15.613544378353634 sem: 0.33457615002693186


1_000_000 particles in pomcpow
zsunberg@tula:~/.julia/v0.5/ContinuousPOMDPTreeSearchExperiments$ julia -p 10  --color=yes scratch/eval_pomcp.jl
heuristic (1 of 5)...100%|██████████████████████████████| Time: 0:00:08
pomcpow (2 of 5)...100%|████████████████████████████████| Time: 0:40:07
modified_pomcp (3 of 5)...100%|█████████████████████████| Time: 0:24:49
vanilla_pomcp (4 of 5)...100%|██████████████████████████| Time: 0:12:51
greedy (5 of 5)...100%|█████████████████████████████████| Time: 0:00:22
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
pomcpow mean: -6.679350450167447 sem: 0.27008009511089115
modified_pomcp mean: -5.909027501851269 sem: 0.2175159081441871
vanilla_pomcp mean: -16.46974571625036 sem: 0.25654700388065066
greedy mean: -15.613544378353634 sem: 0.33457615002693186

switched to SimpleParticleFilter for modified
heuristic (1 of 4)...100%|██████████████████████████████| Time: 0:00:07
modified_pomcp (2 of 4)...100%|█████████████████████████| Time: 0:19:24
vanilla_pomcp (3 of 4)...100%|██████████████████████████| Time: 0:12:58
greedy (4 of 4)...100%|█████████████████████████████████| Time: 0:00:22
heuristic mean: -7.235164761413105 sem: 0.27554282336930086
modified_pomcp mean: -6.267678192270309 sem: 0.18293426883608033
vanilla_pomcp mean: -16.55849811620158 sem: 0.25824830249995767
greedy mean: -15.613544378353634 sem: 0.33457615002693186

Mon 06 Feb 2017 12:15:06 PM PST

[ ] Make parallel script for just one solver
[ ] Test it
[ ] Profile?
[ ] Wait for Particle Filter update

